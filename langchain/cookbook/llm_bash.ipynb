{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bash chain\n",
    "This notebook showcases using LLMs and a bash process to perform simple filesystem commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
      "Please write a bash script that prints 'Hello World' to the console.\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "```bash\n",
      "echo \"Hello World\"\n",
      "```\u001b[0m\n",
      "Code: \u001b[33;1m\u001b[1;3m['echo \"Hello World\"']\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3mHello World\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello World\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain_experimental.llm_bash.base import LLMBashChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "text = \"Please write a bash script that prints 'Hello World' to the console.\"\n",
    "\n",
    "bash_chain = LLMBashChain.from_llm(llm, verbose=True)\n",
    "\n",
    "bash_chain.run(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Prompt\n",
    "You can also customize the prompt that is used. Here is an example prompting to avoid using the 'echo' utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llm_bash.prompt import BashOutputParser\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"If someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\n",
    "Question: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\n",
    "I need to take the following actions:\n",
    "- List all files in the directory\n",
    "- Create a new directory\n",
    "- Copy the files from the first directory into the second directory\n",
    "```bash\n",
    "ls\n",
    "mkdir myNewDirectory\n",
    "cp -r target/* myNewDirectory\n",
    "```\n",
    "\n",
    "Do not use 'echo' when writing the script.\n",
    "\n",
    "That is the format. Begin!\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=_PROMPT_TEMPLATE,\n",
    "    output_parser=BashOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
      "Please write a bash script that prints 'Hello World' to the console.\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "```bash\n",
      "printf \"Hello World\\n\"\n",
      "```\u001b[0m\n",
      "Code: \u001b[33;1m\u001b[1;3m['printf \"Hello World\\\\n\"']\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3mHello World\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello World\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bash_chain = LLMBashChain.from_llm(llm, prompt=PROMPT, verbose=True)\n",
    "\n",
    "text = \"Please write a bash script that prints 'Hello World' to the console.\"\n",
    "\n",
    "bash_chain.run(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent Terminal\n",
    "\n",
    "By default, the chain will run in a separate subprocess each time it is called. This behavior can be changed by instantiating with a persistent bash process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
      "List the current directory then move up a level.\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "```bash\n",
      "ls\n",
      "cd ..\n",
      "```\u001b[0m\n",
      "Code: \u001b[33;1m\u001b[1;3m['ls', 'cd ..']\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3mLLaMA2_sql_chat.ipynb\n",
      "Multi_modal_RAG.ipynb\n",
      "README.md\n",
      "Semi_Structured_RAG.ipynb\n",
      "Semi_structured_and_multi_modal_RAG.ipynb\n",
      "Semi_structured_multi_modal_RAG_LLaMA2.ipynb\n",
      "advanced_rag_eval.ipynb\n",
      "analyze_document.ipynb\n",
      "autogpt\n",
      "baby_agi.ipynb\n",
      "baby_agi_with_agent.ipynb\n",
      "camel_role_playing.ipynb\n",
      "causal_program_aided_language_model.ipynb\n",
      "code-analysis-deeplake.ipynb\n",
      "custom_agent_with_plugin_retrieval.ipynb\n",
      "custom_agent_with_plugin_retrieval_using_plugnplai.ipynb\n",
      "databricks_sql_db.ipynb\n",
      "deeplake_semantic_search_over_chat.ipynb\n",
      "elasticsearch_db_qa.ipynb\n",
      "extraction_openai_tools.ipynb\n",
      "fake_llm.ipynb\n",
      "forward_looking_retrieval_augmented_generation.ipynb\n",
      "generative_agents_interactive_simulacra_of_human_behavior.ipynb\n",
      "gymnasium_agent_simulation.ipynb\n",
      "hugginggpt.ipynb\n",
      "human_input_chat_model.ipynb\n",
      "human_input_llm.ipynb\n",
      "hypothetical_document_embeddings.ipynb\n",
      "learned_prompt_optimization.ipynb\n",
      "llm_bash.ipynb\n",
      "llm_checker.ipynb\n",
      "llm_math.ipynb\n",
      "llm_summarization_checker.ipynb\n",
      "llm_symbolic_math.ipynb\n",
      "meta_prompt.ipynb\n",
      "multi_modal_QA.ipynb\n",
      "multi_modal_RAG_chroma.ipynb\n",
      "multi_modal_output_agent.ipynb\n",
      "multi_player_dnd.ipynb\n",
      "multiagent_authoritarian.ipynb\n",
      "multiagent_bidding.ipynb\n",
      "myscale_vector_sql.ipynb\n",
      "openai_functions_retrieval_qa.ipynb\n",
      "openai_v1_cookbook.ipynb\n",
      "petting_zoo.ipynb\n",
      "plan_and_execute_agent.ipynb\n",
      "press_releases.ipynb\n",
      "program_aided_language_model.ipynb\n",
      "qa_citations.ipynb\n",
      "qianfan_baidu_elasticesearch_RAG.ipynb\n",
      "rag_fusion.ipynb\n",
      "retrieval_in_sql.ipynb\n",
      "rewrite.ipynb\n",
      "sales_agent_with_context.ipynb\n",
      "selecting_llms_based_on_context_length.ipynb\n",
      "self_query_hotel_search.ipynb\n",
      "smart_llm.ipynb\n",
      "sql_db_qa.mdx\n",
      "stepback-qa.ipynb\n",
      "tree_of_thought.ipynb\n",
      "twitter-the-algorithm-analysis-deeplake.ipynb\n",
      "two_agent_debate_tools.ipynb\n",
      "two_player_dnd.ipynb\n",
      "wikibase_agent.ipynb\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LLaMA2_sql_chat.ipynb\\r\\nMulti_modal_RAG.ipynb\\r\\nREADME.md\\r\\nSemi_Structured_RAG.ipynb\\r\\nSemi_structured_and_multi_modal_RAG.ipynb\\r\\nSemi_structured_multi_modal_RAG_LLaMA2.ipynb\\r\\nadvanced_rag_eval.ipynb\\r\\nanalyze_document.ipynb\\r\\nautogpt\\r\\nbaby_agi.ipynb\\r\\nbaby_agi_with_agent.ipynb\\r\\ncamel_role_playing.ipynb\\r\\ncausal_program_aided_language_model.ipynb\\r\\ncode-analysis-deeplake.ipynb\\r\\ncustom_agent_with_plugin_retrieval.ipynb\\r\\ncustom_agent_with_plugin_retrieval_using_plugnplai.ipynb\\r\\ndatabricks_sql_db.ipynb\\r\\ndeeplake_semantic_search_over_chat.ipynb\\r\\nelasticsearch_db_qa.ipynb\\r\\nextraction_openai_tools.ipynb\\r\\nfake_llm.ipynb\\r\\nforward_looking_retrieval_augmented_generation.ipynb\\r\\ngenerative_agents_interactive_simulacra_of_human_behavior.ipynb\\r\\ngymnasium_agent_simulation.ipynb\\r\\nhugginggpt.ipynb\\r\\nhuman_input_chat_model.ipynb\\r\\nhuman_input_llm.ipynb\\r\\nhypothetical_document_embeddings.ipynb\\r\\nlearned_prompt_optimization.ipynb\\r\\nllm_bash.ipynb\\r\\nllm_checker.ipynb\\r\\nllm_math.ipynb\\r\\nllm_summarization_checker.ipynb\\r\\nllm_symbolic_math.ipynb\\r\\nmeta_prompt.ipynb\\r\\nmulti_modal_QA.ipynb\\r\\nmulti_modal_RAG_chroma.ipynb\\r\\nmulti_modal_output_agent.ipynb\\r\\nmulti_player_dnd.ipynb\\r\\nmultiagent_authoritarian.ipynb\\r\\nmultiagent_bidding.ipynb\\r\\nmyscale_vector_sql.ipynb\\r\\nopenai_functions_retrieval_qa.ipynb\\r\\nopenai_v1_cookbook.ipynb\\r\\npetting_zoo.ipynb\\r\\nplan_and_execute_agent.ipynb\\r\\npress_releases.ipynb\\r\\nprogram_aided_language_model.ipynb\\r\\nqa_citations.ipynb\\r\\nqianfan_baidu_elasticesearch_RAG.ipynb\\r\\nrag_fusion.ipynb\\r\\nretrieval_in_sql.ipynb\\r\\nrewrite.ipynb\\r\\nsales_agent_with_context.ipynb\\r\\nselecting_llms_based_on_context_length.ipynb\\r\\nself_query_hotel_search.ipynb\\r\\nsmart_llm.ipynb\\r\\nsql_db_qa.mdx\\r\\nstepback-qa.ipynb\\r\\ntree_of_thought.ipynb\\r\\ntwitter-the-algorithm-analysis-deeplake.ipynb\\r\\ntwo_agent_debate_tools.ipynb\\r\\ntwo_player_dnd.ipynb\\r\\nwikibase_agent.ipynb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.llm_bash.bash import BashProcess\n",
    "\n",
    "persistent_process = BashProcess(persistent=True)\n",
    "bash_chain = LLMBashChain.from_llm(llm, bash_process=persistent_process, verbose=True)\n",
    "\n",
    "text = \"List the current directory then move up a level.\"\n",
    "\n",
    "bash_chain.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
      "List the current directory then move up a level.\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "```bash\n",
      "ls\n",
      "cd ..\n",
      "```\u001b[0m\n",
      "Code: \u001b[33;1m\u001b[1;3m['ls', 'cd ..']\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3mcallback.ipynb\t\t\tlangchain-basic.ipynb\n",
      "cookbook\t\t\tllmonitor.ipynb\n",
      "few-shot-chat-model.ipynb\twatsonx-langchain-rag.ipynb\n",
      "few-shot-prompt.ipynb\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'callback.ipynb\\t\\t\\tlangchain-basic.ipynb\\r\\ncookbook\\t\\t\\tllmonitor.ipynb\\r\\nfew-shot-chat-model.ipynb\\twatsonx-langchain-rag.ipynb\\r\\nfew-shot-prompt.ipynb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the same command again and see that the state is maintained between calls\n",
    "bash_chain.run(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

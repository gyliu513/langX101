{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70996d8a",
   "metadata": {},
   "source": [
    "# WatsonxLLM\n",
    "\n",
    "[WatsonxLLM](https://ibm.github.io/watson-machine-learning-sdk/fm_extensions.html) is wrapper for IBM [watsonx.ai](https://www.ibm.com/products/watsonx-ai) foundation models.\n",
    "This example shows how to communicate with watsonx.ai models using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35b2b7",
   "metadata": {},
   "source": [
    "Install the package [`ibm_watson_machine_learning`](https://ibm.github.io/watson-machine-learning-sdk/install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1fff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ibm_watson_machine_learning in /Users/gyliu/py311/lib/python3.11/site-packages (1.0.330)\n",
      "Requirement already satisfied: requests in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (2.31.0)\n",
      "Requirement already satisfied: urllib3 in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (1.26.18)\n",
      "Requirement already satisfied: pandas<1.6.0,>=0.24.2 in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (1.5.3)\n",
      "Requirement already satisfied: certifi in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (2023.7.22)\n",
      "Requirement already satisfied: lomond in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (0.3.3)\n",
      "Requirement already satisfied: tabulate in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (0.9.0)\n",
      "Requirement already satisfied: packaging in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (23.2)\n",
      "Requirement already satisfied: importlib-metadata in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (6.8.0)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm_watson_machine_learning) (2.13.2)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.13.2 in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning) (2.13.2)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.2 in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning) (2.13.2)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /Users/gyliu/py311/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.13.2->ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gyliu/py311/lib/python3.11/site-packages (from pandas<1.6.0,>=0.24.2->ibm_watson_machine_learning) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/gyliu/py311/lib/python3.11/site-packages (from pandas<1.6.0,>=0.24.2->ibm_watson_machine_learning) (1.26.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/gyliu/py311/lib/python3.11/site-packages (from requests->ibm_watson_machine_learning) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gyliu/py311/lib/python3.11/site-packages (from requests->ibm_watson_machine_learning) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/gyliu/py311/lib/python3.11/site-packages (from importlib-metadata->ibm_watson_machine_learning) (3.17.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/gyliu/py311/lib/python3.11/site-packages (from lomond->ibm_watson_machine_learning) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ibm_watson_machine_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406e092",
   "metadata": {},
   "source": [
    "This cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n",
    "\n",
    "**Action:** Provide the IBM Cloud user API key. For details, see\n",
    "[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d572a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WATSONX_APIKEY\"] = os.getenv(\"IAM_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36acbef",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "You might need to adjust model `parameters` for different models or tasks, to do so please refer to [documentation](https://ibm.github.io/watson-machine-learning-sdk/model.html#metanames.GenTextParamsMetaNames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407cd500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b586538",
   "metadata": {},
   "source": [
    "Initialize the `WatsonxLLM` class with previous set params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359898de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import WatsonxLLM\n",
    "\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.getenv(\"PROJECT_ID\"),\n",
    "    params=parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202f4e0",
   "metadata": {},
   "source": [
    "Alternatively you can use Cloud Pak for Data credentials. For details, see [documentation](https://ibm.github.io/watson-machine-learning-sdk/setup_cpd.html).\n",
    "```\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id='google/flan-ul2',\n",
    "    url=\"***\",\n",
    "    username=\"***\",\n",
    "    password=\"***\",\n",
    "    instance_id=\"openshift\",\n",
    "    version=\"4.8\",\n",
    "    project_id='***',\n",
    "    params=parameters\n",
    ")\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25ecbd1",
   "metadata": {},
   "source": [
    "## Create Chain\n",
    "Create `PromptTemplate` objects which will be responsible for creating a random question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d80c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"Generate a random question about {topic}: Question: \"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79056d8e",
   "metadata": {},
   "source": [
    "Provide a topic and run the `LLMChain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc076c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the term for a dog with a long tail?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=watsonx_llm)\n",
    "llm_chain.run(\"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f571001d",
   "metadata": {},
   "source": [
    "## Calling the Model Directly\n",
    "To obtain completions, you can can the model directly using string prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beea2b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling a single prompt\n",
    "\n",
    "watsonx_llm(\"Who is man's best friend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ab1a25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='lassie', generation_info={'finish_reason': 'eos_token'})], [Generation(text='The Basenji is a breed of dog from South Africa.', generation_info={'finish_reason': 'eos_token'})]], llm_output={'token_usage': {'generated_token_count': 18, 'input_token_count': 15}, 'model_id': 'google/flan-ul2'}, run=[RunInfo(run_id=UUID('1f038d01-a158-42ca-a7ef-90bbcaf092bc')), RunInfo(run_id=UUID('6dc7f8b3-8c8e-42e7-a0e9-dc892f916b44'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling multiple prompts\n",
    "\n",
    "watsonx_llm.generate(\n",
    "    [\n",
    "        \"The fastest dog in the world?\",\n",
    "        \"Describe your chosen dog breed\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9da33",
   "metadata": {},
   "source": [
    "## Streaming the Model output \n",
    "\n",
    "You can stream the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f63166a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Doberman pinscher is a great dog."
     ]
    }
   ],
   "source": [
    "for chunk in watsonx_llm.stream(\n",
    "    \"Describe your favorite breed of dog and why it is your favorite.\"\n",
    "):\n",
    "    print(chunk, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

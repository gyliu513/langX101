# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the terms described in the LICENSE file in
# the root directory of this source tree.

import tempfile
import time
from pathlib import Path
from urllib.request import urlopen

import fire
from llama_stack_client import LlamaStackClient
from termcolor import colored

from .utils import check_model_is_available, get_any_available_model


def _get_model_type(model) -> str | None:
    for attr in ("model_type", "type", "model_kind", "kind", "model_family"):
        value = getattr(model, attr, None)
        if isinstance(value, str):
            return value
    for metadata_attr in ("custom_metadata", "metadata"):
        metadata = getattr(model, metadata_attr, None)
        if isinstance(metadata, dict):
            value = metadata.get("model_type") or metadata.get("type")
            if isinstance(value, str):
                return value
    return None


def _get_model_id(model) -> str | None:
    for attr in ("identifier", "model_id", "id", "name"):
        value = getattr(model, attr, None)
        if isinstance(value, str):
            return value
    return None


def _get_any_available_embedding_model(client: LlamaStackClient) -> str | None:
    embedding_models = [
        model_id
        for model in client.models.list()
        for model_id in [_get_model_id(model)]
        if model_id
        and (
            _get_model_type(model) == "embedding"
            or "embedding" in model_id.lower()
            or "embed" in model_id.lower()
        )
    ]
    if not embedding_models:
        print(colored("No available embedding models.", "red"))
        return None
    return embedding_models[0]


def _get_embedding_dimension(client: LlamaStackClient, model_id: str) -> int | None:
    try:
        response = client.embeddings.create(model=model_id, input="dimension probe")
    except Exception:
        return None
    if not response.data:
        return None
    embedding = response.data[0].embedding
    if isinstance(embedding, list):
        return len(embedding)
    return None


def _download_documents(urls: list[str], target_dir: Path) -> list[Path]:
    local_paths: list[Path] = []
    for url in urls:
        filename = url.rsplit("/", 1)[-1]
        target_path = target_dir / filename
        try:
            with urlopen(url) as response:
                content_bytes = response.read()
        except Exception as exc:
            print(colored(f"Failed to download {url}: {exc}", "red"))
            continue
        try:
            content_text = content_bytes.decode("utf-8", errors="ignore").strip()
        except Exception as exc:
            print(colored(f"Failed to decode {url}: {exc}", "red"))
            continue
        if not content_text:
            print(colored(f"Downloaded empty content from {url}", "red"))
            continue
        target_path = target_path.with_suffix(".txt")
        target_path.write_text(content_text, encoding="utf-8")
        local_paths.append(target_path)
    return local_paths


def _build_context(search_results) -> str:
    if not search_results:
        return ""
    context_lines = ["Context from uploaded documents:"]
    for result in search_results:
        snippet = " ".join(
            content.text.strip() for content in result.content if getattr(content, "text", None)
        ).strip()
        if not snippet:
            continue
        context_lines.append(f"- {result.filename} (score={result.score:.2f}): {snippet}")
    return "\n".join(context_lines)


def main(
    host: str,
    port: int,
    model_id: str | None = None,
    embedding_model_id: str | None = None,
):
    urls = [
        "https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/memory_optimizations.rst",
        "https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/chat.rst",
        "https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/llama3.rst",
        "https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/qat_finetune.rst",
        "https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/lora_finetune.rst",
    ]

    client = LlamaStackClient(
        base_url=f"http://{host}:{port}",
    )

    if model_id is None:
        model_id = get_any_available_model(client)
        if model_id is None:
            return
    else:
        if not check_model_is_available(client, model_id):
            return

    print(f"Using model: {model_id}")

    embedding_model = embedding_model_id or _get_any_available_embedding_model(client)
    if embedding_model is None:
        return

    embedding_dimension = _get_embedding_dimension(client, embedding_model)
    if embedding_dimension is None:
        print(colored("Unable to determine embedding dimension.", "red"))
        return

    vector_store = client.vector_stores.create(
        name="chat-with-documents",
        extra_body={"embedding_model": embedding_model, "embedding_dimension": embedding_dimension},
    )

    file_ids: list[str] = []
    attached_file_ids: list[str] = []
    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            doc_paths = _download_documents(urls, Path(tmpdir))
            if not doc_paths:
                print(colored("No documents downloaded. Exiting.", "red"))
                return
            for doc_path in doc_paths:
                with open(doc_path, "rb") as doc_file:
                    file_response = client.files.create(file=doc_file, purpose="assistants")
                file_ids.append(file_response.id)

                file_attach_response = client.vector_stores.files.create(
                    vector_store_id=vector_store.id,
                    file_id=file_response.id,
                    chunking_strategy={
                        "type": "static",
                        "static": {
                            "max_chunk_size_tokens": 512,
                            "chunk_overlap_tokens": 64,
                        },
                    },
                )
                attach_start = time.time()
                while file_attach_response.status == "in_progress":
                    time.sleep(0.2)
                    if time.time() - attach_start > 30:
                        break
                    file_attach_response = client.vector_stores.files.retrieve(
                        vector_store_id=vector_store.id,
                        file_id=file_response.id,
                    )
                if file_attach_response.status != "completed":
                    error_message = None
                    if file_attach_response.last_error:
                        error_message = file_attach_response.last_error.message
                    if time.time() - attach_start > 30 and file_attach_response.status == "in_progress":
                        print(colored(f"Timed out attaching {doc_path.name}", "red"))
                    else:
                        print(
                            colored(
                                f"Failed to attach {doc_path.name}"
                                + (f": {error_message}" if error_message else ""),
                                "red",
                            )
                        )
                    continue
                attached_file_ids.append(file_response.id)

            if not attached_file_ids:
                print(colored("No documents attached to the vector store. Exiting.", "red"))
                return

        conversation = client.conversations.create(metadata={"name": "test-session"})
        session_id = conversation.id
        print(f"Created session_id={session_id}")

        user_prompts = [
            "I uploaded Torchtune documentation for reference.",
            "What are the top 5 topics that were explained? Only list succinct bullet points.",
            "Was anything related to 'Llama3' discussed, if so what?",
            "Tell me how to use LoRA",
            "What about Quantization?",
        ]

        for prompt in user_prompts:
            print(colored(f"User> {prompt}", "blue"))
            search_response = client.vector_stores.search(
                vector_store_id=vector_store.id,
                query=prompt,
                max_num_results=5,
            )
            context = _build_context(search_response.data)
            instructions = "You are a helpful assistant."
            if context:
                instructions = f"{instructions}\n\n{context}"

            response = client.responses.create(
                model=model_id,
                conversation=session_id,
                instructions=instructions,
                input=[{"role": "user", "content": prompt}],
                stream=False,
            )
            print(response.output_text)
    finally:
        try:
            client.vector_stores.delete(vector_store_id=vector_store.id)
        except Exception:
            pass
        for file_id in file_ids:
            try:
                client.files.delete(file_id=file_id)
            except Exception:
                pass


if __name__ == "__main__":
    fire.Fire(main)

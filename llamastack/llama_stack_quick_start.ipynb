{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1e7571c"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb)\n",
    "\n",
    "# Llama Stack - Building AI Applications\n",
    "\n",
    "<img src=\"https://llamastack.github.io/latest/_images/llama-stack.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Get started with Llama Stack in minutes!\n",
    "\n",
    "[Llama Stack](https://github.com/meta-llama/llama-stack) is a stateful service with REST APIs to support the seamless transition of AI applications across different environments. You can build and test using a local server first and deploy to a hosted endpoint for production.\n",
    "\n",
    "In this guide, we'll walk through how to build a RAG application locally using Llama Stack with [Ollama](https://ollama.com/)\n",
    "as the inference [provider](docs/source/providers/index.md#inference) for a Llama Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CV1Q19BDMVw"
   },
   "source": [
    "## Step 1: Install and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4AvfUAJZOeS"
   },
   "source": [
    "### 1.1. Install uv and test inference with Ollama\n",
    "\n",
    "We'll install [uv](https://docs.astral.sh/uv/) to setup the Python virtual environment, along with [colab-xterm](https://github.com/InfuseAI/colab-xterm) for running command-line tools, and [Ollama](https://ollama.com/download) as the inference provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (0.9.18)\n",
      "Requirement already satisfied: llama_stack in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (0.2.12)\n",
      "Requirement already satisfied: llama-stack-client in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (0.2.12)\n",
      "Requirement already satisfied: aiohttp in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (3.13.2)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (0.125.0)\n",
      "Requirement already satisfied: fire in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (0.7.1)\n",
      "Requirement already satisfied: httpx in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (1.2.3)\n",
      "Requirement already satisfied: jinja2>=3.1.6 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.66 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (2.14.0)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (3.0.52)\n",
      "Requirement already satisfied: python-dotenv in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (1.2.1)\n",
      "Requirement already satisfied: python-jose in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (3.5.0)\n",
      "Requirement already satisfied: pydantic>=2 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (2.12.5)\n",
      "Requirement already satisfied: requests in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (2.32.5)\n",
      "Requirement already satisfied: rich in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (14.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (65.5.0)\n",
      "Requirement already satisfied: starlette in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (0.50.0)\n",
      "Requirement already satisfied: termcolor in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (3.2.0)\n",
      "Requirement already satisfied: tiktoken in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (0.12.0)\n",
      "Requirement already satisfied: pillow in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (12.0.0)\n",
      "Requirement already satisfied: h11>=0.16.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (0.16.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.20 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama_stack) (0.0.21)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (4.12.0)\n",
      "Requirement already satisfied: click in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (8.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (1.9.0)\n",
      "Requirement already satisfied: pandas in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (2.3.3)\n",
      "Requirement already satisfied: pyaml in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (25.7.0)\n",
      "Requirement already satisfied: sniffio in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from llama-stack-client) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from anyio<5,>=3.5.0->llama-stack-client) (3.11)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from fastapi<1.0,>=0.115.0->llama_stack) (0.0.4)\n",
      "Requirement already satisfied: certifi in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from httpx->llama_stack) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from httpx->llama_stack) (1.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from jinja2>=3.1.6->llama_stack) (3.0.3)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from openai>=1.66->llama_stack) (0.12.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from pydantic>=2->llama_stack) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from pydantic>=2->llama_stack) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from pydantic>=2->llama_stack) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from aiohttp->llama_stack) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from aiohttp->llama_stack) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from aiohttp->llama_stack) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from aiohttp->llama_stack) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from aiohttp->llama_stack) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from aiohttp->llama_stack) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from aiohttp->llama_stack) (1.22.0)\n",
      "Requirement already satisfied: filelock in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from huggingface-hub->llama_stack) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from huggingface-hub->llama_stack) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from huggingface-hub->llama_stack) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from huggingface-hub->llama_stack) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from huggingface-hub->llama_stack) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from huggingface-hub->llama_stack) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from huggingface-hub->llama_stack) (0.20.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from jsonschema->llama_stack) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from jsonschema->llama_stack) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from jsonschema->llama_stack) (0.30.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from pandas->llama-stack-client) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from pandas->llama-stack-client) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from pandas->llama-stack-client) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from pandas->llama-stack-client) (2025.3)\n",
      "Requirement already satisfied: wcwidth in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from prompt-toolkit->llama_stack) (0.2.14)\n",
      "Requirement already satisfied: ecdsa!=0.15 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from python-jose->llama_stack) (0.19.1)\n",
      "Requirement already satisfied: rsa!=4.1.1,!=4.4,<5.0,>=4.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from python-jose->llama_stack) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.5.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from python-jose->llama_stack) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from requests->llama_stack) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from requests->llama_stack) (2.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from rich->llama_stack) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from rich->llama_stack) (2.19.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from tiktoken->llama_stack) (2025.11.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from ecdsa!=0.15->python-jose->llama_stack) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/gyliu-cary/ycliu/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ“ Ollama is already installed\n",
      "  Version: ollama version is 0.13.5\n"
     ]
    }
   ],
   "source": [
    "%pip install uv llama_stack llama-stack-client\n",
    "\n",
    "## If running on Collab:\n",
    "# !pip install colab-xterm\n",
    "# %load_ext colabxterm\n",
    "\n",
    "import platform\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Check if Ollama is already installed\n",
    "if shutil.which(\"ollama\"):\n",
    "    print(\"âœ“ Ollama is already installed\")\n",
    "    # Verify it works\n",
    "    result = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  Version: {result.stdout.strip()}\")\n",
    "else:\n",
    "    # Install Ollama based on the platform\n",
    "    system = platform.system()\n",
    "    if system == \"Darwin\":  # macOS\n",
    "        # Check if Homebrew is available\n",
    "        if shutil.which(\"brew\"):\n",
    "            print(\"Installing Ollama via Homebrew...\")\n",
    "            subprocess.run([\"brew\", \"install\", \"ollama\"], check=True)\n",
    "            print(\"âœ“ Ollama installed successfully\")\n",
    "        else:\n",
    "            print(\"Homebrew not found. Please install Ollama manually from https://ollama.com/download\")\n",
    "            print(\"Or install Homebrew first: /bin/bash -c \\\"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\\\"\")\n",
    "    elif system == \"Linux\":\n",
    "        print(\"Installing Ollama via install script...\")\n",
    "        subprocess.run([\"curl\", \"https://ollama.ai/install.sh\", \"-o\", \"/tmp/install_ollama.sh\"], check=True)\n",
    "        subprocess.run([\"sh\", \"/tmp/install_ollama.sh\"], check=True)\n",
    "        print(\"âœ“ Ollama installed successfully\")\n",
    "    else:\n",
    "        print(f\"Unsupported platform: {system}. Please install Ollama manually from https://ollama.com/download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Test inference with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ll now launch a terminal and run inference on a Llama model with Ollama to verify that the model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If running on Colab:\n",
    "# %xterm\n",
    "\n",
    "## To be ran in the terminal:\n",
    "# ollama serve &\n",
    "# ollama run llama3.2:3b --keepalive 60m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If successful, you should see the model respond to a prompt.\n",
    "\n",
    "...\n",
    "```\n",
    ">>> hi\n",
    "Hello! How can I assist you today?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDUB7M_qe-Gs"
   },
   "source": [
    "## Step 2: Run the Llama Stack server\n",
    "\n",
    "In this showcase, we will start a Llama Stack server that is running locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Setup the Llama Stack Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J2kGed0R5PSf",
    "outputId": "2478ea60-8d35-48a1-b011-f233831740c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gyliu-cary/ycliu/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'default' attribute with value 'sqlite' was provided to the `Field()` function, which has no effect in the context it was used. 'default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "usage: llama stack [-h] [--version]\n",
      "                   {build,list-apis,list-providers,run,rm,list} ...\n",
      "llama stack: error: argument {build,list-apis,list-providers,run,rm,list}: invalid choice: 'list-deps' (choose from 'build', 'list-apis', 'list-providers', 'run', 'rm', 'list')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "if \"UV_SYSTEM_PYTHON\" in os.environ:\n",
    "  del os.environ[\"UV_SYSTEM_PYTHON\"]\n",
    "\n",
    "# this command installs all the dependencies needed for the llama stack server with the ollama inference provider\n",
    "!uv run --with llama-stack llama stack list-deps starter | xargs -L1 uv pip install\n",
    "\n",
    "def run_llama_stack_server_background():\n",
    "    log_file = open(\"llama_stack_server.log\", \"w\")\n",
    "    process = subprocess.Popen(\n",
    "        f\"OLLAMA_URL=http://localhost:11434 uv run --with llama-stack llama stack run starter\",\n",
    "        shell=True,\n",
    "        stdout=log_file,\n",
    "        stderr=log_file,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    print(f\"Starting Llama Stack server with PID: {process.pid}\")\n",
    "    return process\n",
    "\n",
    "def wait_for_server_to_start():\n",
    "    import requests\n",
    "    from requests.exceptions import ConnectionError\n",
    "    import time\n",
    "\n",
    "    url = \"http://0.0.0.0:8321/v1/health\"\n",
    "    max_retries = 30\n",
    "    retry_interval = 1\n",
    "\n",
    "    print(\"Waiting for server to start\", end=\"\")\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                print(\"\\nServer is ready!\")\n",
    "                return True\n",
    "        except ConnectionError:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(retry_interval)\n",
    "\n",
    "    print(\"\\nServer failed to start after\", max_retries * retry_interval, \"seconds\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# use this helper if needed to kill the server\n",
    "def kill_llama_stack_server():\n",
    "    # Kill any existing llama stack server processes\n",
    "    os.system(\"ps aux | grep -v grep | grep llama_stack.core.server.server | awk '{print $2}' | xargs kill -9\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Start the Llama Stack Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Llama Stack server with PID: 20778\n",
      "Waiting for server to start........\n",
      "Server is ready!\n"
     ]
    }
   ],
   "source": [
    "server_process = run_llama_stack_server_background()\n",
    "assert wait_for_server_to_start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: RAG Demos - Three Approaches\n",
    "\n",
    "We'll demonstrate three different approaches to building RAG applications with Llama Stack:\n",
    "1. **Agent API** - High-level agent with session management\n",
    "2. **Responses API** - Direct OpenAI-compatible responses\n",
    "3. **Chat Completions API** - Manual retrieval with explicit control\n",
    "\n",
    "### Approach 1: Agent Class (High-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://0.0.0.0:8321/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['bedrock/meta.llama3-1-405b-instruct-v1:0', 'bedrock/meta.llama3-1-70b-instruct-v1:0', 'bedrock/meta.llama3-1-8b-instruct-v1:0', 'ollama/chevalblanc/gpt-4o-mini:latest', 'ollama/nomic-embed-text:latest', 'ollama/llama3.3:70b', 'ollama/llama3.2:3b', 'ollama/all-minilm:l6-v2', 'ollama/llama3.1:8b', 'ollama/llama-guard3:latest', 'ollama/llama-guard3:8b', 'ollama/shieldgemma:27b', 'ollama/shieldgemma:latest', 'ollama/llama3.1:8b-instruct-fp16', 'ollama/all-minilm:latest', 'ollama/llama3.2:3b-instruct-fp16', 'sentence-transformers/nomic-ai/nomic-embed-text-v1.5']\n",
      "âœ“ Using model: ollama/llama3.3:70b\n",
      "\n",
      "âœ“ Downloading and indexing Paul Graham's essay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/files \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ File created with ID: file-e1290f8be28245e681bdfa5c40a7e7c4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/vector_stores \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/conversations \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Vector store created with ID: vs_67efaaf4-ba0d-4037-b816-f73d588e9e4d\n",
      "âœ“ Agent created\n",
      "\n",
      "prompt> How do you do great work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤” \n",
      "\n",
      "ðŸ”§ Executing file_search (server-side)...\n",
      "ðŸ¤” To do great work it's essential to decide what to work on and choose something you have a natural aptitude for that you are deeply interested in and offers scope to do great work <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. Develop a habit of working on your own projects and don't let \"work\" mean something other people tell you to do <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. Seek out the best colleagues as they can encourage you and help bounce ideas off each other and it's better to have one or two great ones than a building full of pretty good ones <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. Husband your morale as it's crucial for doing great work and try to learn about other kinds of work by taking ideas from distant fields if you let them be metaphors <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. Negative examples can also be inspiring so try to learn from things done badly as sometimes it becomes clear what's needed when it's missing <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. If you're earnest you'll probably get a warmer welcome than expected when visiting places with the best people in your field which can increase your ambition and self-confidence <|file-e1290f8be28245e681bdfa5c40a7e7c4|>.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make sure that your llama stack client version matches with the llama stack server version you are using.\n",
    "from llama_stack_client import Agent, AgentEventLogger, LlamaStackClient\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "vector_store_id = \"my_demo_vector_db\"\n",
    "client = LlamaStackClient(base_url=\"http://0.0.0.0:8321\")\n",
    "\n",
    "# Get model - find any Ollama Llama model\n",
    "models = list(client.models.list())\n",
    "print(f\"Available models: {[m.id for m in models]}\")\n",
    "\n",
    "# Find any Ollama Llama LLM model\n",
    "model_id = None\n",
    "priority_models = [\"ollama/llama3.3:70b\",\"ollama/llama3.2:3b\",\"ollama/llama3.1:8b\"]\n",
    "for m in models:\n",
    "    if hasattr(m, \"custom_metadata\") and m.custom_metadata:\n",
    "        provider_id = m.custom_metadata.get(\"provider_id\")\n",
    "        model_type = m.custom_metadata.get(\"model_type\")\n",
    "\n",
    "        # Use any Ollama LLM model with \"llama\" in the name\n",
    "        if provider_id == \"ollama\" and model_type == \"llm\" and m.id.lower() in priority_models:\n",
    "            model_id = m.id\n",
    "            print(f\"âœ“ Using model: {model_id}\")\n",
    "            break\n",
    "\n",
    "if not model_id:\n",
    "    raise ValueError(\"No Ollama Llama model found\")\n",
    "\n",
    "# Create vector store\n",
    "print(\"\\nâœ“ Downloading and indexing Paul Graham's essay...\")\n",
    "source = \"https://www.paulgraham.com/greatwork.html\"\n",
    "response = requests.get(source)\n",
    "\n",
    "# Create a file-like object from the HTML content\n",
    "file_buffer = BytesIO(response.content)\n",
    "file_buffer.name = \"greatwork.html\"\n",
    "\n",
    "file = client.files.create(\n",
    "    file=file_buffer,\n",
    "    purpose='assistants'\n",
    ")\n",
    "print(f\"âœ“ File created with ID: {file.id}\")\n",
    "\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=vector_store_id,\n",
    "    file_ids=[file.id],\n",
    ")\n",
    "print(f\"âœ“ Vector store created with ID: {vector_store.id}\")\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(\n",
    "    client,\n",
    "    model=model_id,\n",
    "    instructions=\"You are a helpful assistant\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store.id],  # Use the actual ID, not the name\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "print(\"âœ“ Agent created\")\n",
    "\n",
    "prompt = \"How do you do great work?\"\n",
    "print(\"\\nprompt>\", prompt)\n",
    "\n",
    "response = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    session_id=agent.create_session(\"rag_session\"),\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for log in AgentEventLogger().log(response):\n",
    "    print(log, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-turn RAG Conversation with Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/conversations \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Multi-turn RAG Conversation Demo\n",
      "================================================================================\n",
      "Demonstrating: Session maintains context while agent searches document\n",
      "================================================================================\n",
      "\n",
      "[Turn 1] User: What does the document say about curiosity and great work?\n",
      "(Agent will search the document...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: ðŸ¤” \n",
      "\n",
      "ðŸ”§ Executing file_search (server-side)...\n",
      "ðŸ¤” Curiosity is a key factor in doing great work <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. It drives people to learn and explore new ideas, which can lead to innovative solutions and discoveries <|file-e12...\n",
      "\n",
      "[Turn 2] User: Why is that important?\n",
      "(Agent remembers 'that' refers to curiosity from Turn 1 - no need to search again)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: ðŸ¤” \n",
      "\n",
      "ðŸ”§ Executing file_search (server-side)...\n",
      "ðŸ¤” Curiosity plays a crucial role in driving individuals to do great work and make meaningful discoveries <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. It is the key to all four steps in doing great work: choo...\n",
      "\n",
      "[Turn 3] User: What about the role of ambition?\n",
      "(New topic - agent will search document again for 'ambition')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assistant: ðŸ¤” \n",
      "\n",
      "ðŸ”§ Executing file_search (server-side)...\n",
      "ðŸ¤” To do great work it's an advantage to be optimistic even though that means you'll risk looking like a fool sometimes <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. One way to avoid intellectual dishonesty is...\n",
      "\n",
      "[Turn 4] User: How do curiosity and ambition work together?\n",
      "(Agent combines information from Turn 1 and Turn 3 using session context)\n",
      "\n",
      "Assistant: ðŸ¤” \n",
      "\n",
      "ðŸ”§ Executing file_search (server-side)...\n",
      "ðŸ¤” Curiosity and ambition are closely related as they both drive individuals to achieve great work <|file-e1290f8be28245e681bdfa5c40a7e7c4|>. Developing curiosity is essential for doing great work, and it c...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new session for multi-turn RAG conversation\n",
    "session_id = agent.create_session(\"multi_turn_rag_session\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Multi-turn RAG Conversation Demo\")\n",
    "print(\"=\"*80)\n",
    "print(\"Demonstrating: Session maintains context while agent searches document\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Turn 1: Initial question - Agent searches document for relevant information\n",
    "print(\"\\n[Turn 1] User: What does the document say about curiosity and great work?\")\n",
    "print(\"(Agent will search the document...)\")\n",
    "response1 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What does the document say about curiosity and great work?\"}],\n",
    "    session_id=session_id,\n",
    "    stream=True,  # Use streaming for reliability\n",
    ")\n",
    "# Collect the response\n",
    "response1_text = \"\"\n",
    "for log in AgentEventLogger().log(response1):\n",
    "    response1_text += log\n",
    "print(\"\\nAssistant:\", response1_text[:250] + \"...\\n\")\n",
    "\n",
    "# Turn 2: Follow-up question using pronouns - Agent remembers the context from Turn 1\n",
    "print(\"[Turn 2] User: Why is that important?\")\n",
    "print(\"(Agent remembers 'that' refers to curiosity from Turn 1 - no need to search again)\")\n",
    "response2 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is that important?\"}],\n",
    "    session_id=session_id,\n",
    "    stream=True,  # Use streaming for reliability\n",
    ")\n",
    "response2_text = \"\"\n",
    "for log in AgentEventLogger().log(response2):\n",
    "    response2_text += log\n",
    "print(\"\\nAssistant:\", response2_text[:250] + \"...\\n\")\n",
    "\n",
    "# Turn 3: New question on different topic - Agent performs new document search\n",
    "print(\"[Turn 3] User: What about the role of ambition?\")\n",
    "print(\"(New topic - agent will search document again for 'ambition')\")\n",
    "response3 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What about the role of ambition?\"}],\n",
    "    session_id=session_id,\n",
    "    stream=True,  # Use streaming for reliability\n",
    ")\n",
    "response3_text = \"\"\n",
    "for log in AgentEventLogger().log(response3):\n",
    "    response3_text += log\n",
    "print(\"\\nAssistant:\", response3_text[:250] + \"...\\n\")\n",
    "\n",
    "# Turn 4: Compare previous topics - Agent uses session memory\n",
    "print(\"[Turn 4] User: How do curiosity and ambition work together?\")\n",
    "print(\"(Agent combines information from Turn 1 and Turn 3 using session context)\")\n",
    "response4 = agent.create_turn(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do curiosity and ambition work together?\"}],\n",
    "    session_id=session_id,\n",
    "    stream=True,  # Use streaming for reliability\n",
    ")\n",
    "response4_text = \"\"\n",
    "for log in AgentEventLogger().log(response4):\n",
    "    response4_text += log\n",
    "print(\"\\nAssistant:\", response4_text[:250] + \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Chat Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/vector_stores/vs_67efaaf4-ba0d-4037-b816-f73d588e9e4d/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What does paul graham say about curiosity and great work?\n",
      "\n",
      "Searching vector store...\n",
      "Using vector store ID: vs_67efaaf4-ba0d-4037-b816-f73d588e9e4d\n",
      "Extracting context from search results...\n",
      "Found 3 relevant chunks\n",
      "\n",
      "Response (Chat Completions API):\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://0.0.0.0:8321/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Paul Graham, curiosity is a crucial factor in doing great work. He emphasizes that curiosity is the best guide for finding something worth working on, and it plays a significant role in all four steps of doing great work: choosing a field, getting to the frontier, noticing gaps, and exploring them.\n",
      "\n",
      "Graham notes that curiosity is not something that can be commanded, but it can be nurtured and allowed to drive one's efforts. He suggests that curious people are more likely to find the right thing to work on in the first place, as they cast a wide net and are more likely to stumble upon something important.\n",
      "\n",
      "Graham also highlights the importance of curiosity in overcoming obstacles and staying motivated. He argues that when working on something that sparks genuine curiosity, the work will feel less burdensome, even if it's challenging. This is because curious people are driven by a desire to learn and understand, rather than just seeking external validation or rewards.\n",
      "\n",
      "Furthermore, Graham emphasizes that curiosity is a key factor in distinguishing between great work and mediocre work. He notes that people who are truly curious about their work are more likely to produce something original and innovative, whereas those who lack curiosity may simply be going through the motions.\n",
      "\n",
      "In fact, Graham is so convinced of the importance of curiosity that he suggests it might be the single most important factor in doing great work. He even goes so far as to say that if an oracle were to give a single-word answer to the question of how to do great work, it would be \"curiosity.\"\n",
      "\n",
      "Overall, Paul Graham's writings suggest that curiosity is essential for doing great work, and that it plays a central role in driving innovation, creativity, and progress. By nurturing curiosity and allowing it to guide their efforts, individuals can increase their chances of producing something truly remarkable and making a meaningful contribution to their field.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Search vector store explicitly\n",
    "prompt = \"What does paul graham say about curiosity and great work?\"\n",
    "print(f\"User Query: {prompt}\")\n",
    "print(\"\\nSearching vector store...\")\n",
    "print(f\"Using vector store ID: {vector_store.id}\")\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,  # Use the actual ID, not the name\n",
    "    query=prompt,\n",
    "    max_num_results=3,\n",
    "    rewrite_query=False\n",
    ")\n",
    "\n",
    "# Step 2: Extract context from search results\n",
    "print(\"Extracting context from search results...\")\n",
    "context_chunks = []\n",
    "for result in search_results.data:\n",
    "    if hasattr(result, \"content\") and result.content:\n",
    "        for content_item in result.content:\n",
    "            if hasattr(content_item, \"text\") and content_item.text:\n",
    "                context_chunks.append(content_item.text)\n",
    "\n",
    "context = \"\\n\\n\".join(context_chunks)\n",
    "print(f\"Found {len(context_chunks)} relevant chunks\\n\")\n",
    "\n",
    "# Step 3: Use Chat Completions with retrieved context\n",
    "print(\"Response (Chat Completions API):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Use the provided context to answer the user's question.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context:\\n{context}\\n\\nQuestion: {prompt}\\n\\nPlease provide a comprehensive answer based on the context above.\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to dive deeper into Llama Stack!\n",
    "- Explore the [Detailed Tutorial](./detailed_tutorial.md).\n",
    "- Try the [Getting Started Notebook](https://github.com/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb).\n",
    "- Browse more [Notebooks on GitHub](https://github.com/meta-llama/llama-stack/tree/main/docs/notebooks).\n",
    "- Learn about Llama Stack [Concepts](../concepts/index.md).\n",
    "- Discover how to [Build Llama Stacks](../distributions/index.md).\n",
    "- Refer to our [References](../references/index.md) for details on the Llama CLI and Python SDK.\n",
    "- Check out the [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples) repository for example applications and tutorials."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "fileHeader": "",
  "fileUid": "92b9a30f-53a7-4b4f-8cfa-0fed1619256f",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "ycliu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

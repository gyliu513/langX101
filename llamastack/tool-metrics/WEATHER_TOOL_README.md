# Custom Weather Tool with Metrics

è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ llama-stack å·¥å…·å¹¶ç›‘æ§å…¶ metrics çš„å®Œæ•´ç¤ºä¾‹ã€‚

## ğŸ“‹ æ¦‚è¿°

æœ¬ç¤ºä¾‹åŒ…å«ï¼š

1. **è‡ªå®šä¹‰å¤©æ°”å·¥å…·** - ä¸€ä¸ªç®€å•çš„ mock å¤©æ°”æŸ¥è¯¢å·¥å…·
2. **Tool Runtime Metrics** - è‡ªåŠ¨è®°å½•å·¥å…·è°ƒç”¨çš„ metrics
3. **æµ‹è¯•å®¢æˆ·ç«¯** - è°ƒç”¨å¤©æ°”å·¥å…·å¹¶ç”Ÿæˆ metrics çš„è„šæœ¬
4. **ç›‘æ§æ ˆ** - Prometheus + Grafana å¯è§†åŒ– metrics

## ğŸ—ï¸ æ¶æ„

```
test_weather_tool.py (Client)
  â†“ invoke_tool("get_weather", {"city": "Tokyo"})
WeatherToolProvider
  â†“ é€šè¿‡ ToolRuntimeRouter (å·²æ’æ¡©!)
  â†“ è®°å½• metrics: invocations_total, duration_seconds
è¿”å›å¤©æ°”æ•°æ®
  â†“
Metrics å¯¼å‡ºåˆ° OTLP Collector
  â†“ æ¯ 5 ç§’
Prometheus æŠ“å– metrics
  â†“ æ¯ 15 ç§’
Grafana æ˜¾ç¤ºåœ¨ dashboard
```

## ğŸ“ æ–‡ä»¶è¯´æ˜

```
examples/metrics-demo/
  â”œâ”€â”€ weather_tool/
  â”‚   â”œâ”€â”€ __init__.py               - Package åˆå§‹åŒ–
  â”‚   â””â”€â”€ weather_provider.py       - å¤©æ°”å·¥å…·å®ç°
  â”œâ”€â”€ test_weather_tool.py          - æµ‹è¯•å®¢æˆ·ç«¯
  â”œâ”€â”€ quick-test-weather-tool.sh    - ä¸€é”®å¯åŠ¨è„šæœ¬
  â”œâ”€â”€ weather-tool-config.yaml      - Server æ¨¡å¼é…ç½® (å¯é€‰)
  â””â”€â”€ WEATHER_TOOL_README.md        - æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: ä¸€é”®å¯åŠ¨ (æ¨è)

```bash
cd examples/metrics-demo
./quick-test-weather-tool.sh
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
- æ£€æŸ¥å¹¶å¯åŠ¨ Docker ç›‘æ§æ ˆ
- é…ç½®ç¯å¢ƒå˜é‡
- ä½¿ç”¨ library æ¨¡å¼è¿è¡Œæµ‹è¯• (æ— éœ€ server)
- æ‰“å¼€ Grafana å’Œ Prometheus UI

### æ–¹æ³• 2: æ‰‹åŠ¨è¿è¡Œ

```bash
cd examples/metrics-demo

# 1. å¯åŠ¨ç›‘æ§æ ˆ
docker-compose up -d

# 2. é…ç½®ç¯å¢ƒå˜é‡
export USE_SERVER_MODE="false"
export OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4318"
export OTEL_EXPORTER_OTLP_PROTOCOL="http/protobuf"
export OTEL_SERVICE_NAME="llama-stack-weather-tool"

# 3. è¿è¡Œæµ‹è¯•
python test_weather_tool.py
```

## ğŸ”§ å·¥å…·å®ç°è¯¦è§£

### WeatherToolProvider ç±»

```python
class WeatherToolProvider(ToolGroupsProtocolPrivate, ToolRuntime):
    """è‡ªå®šä¹‰å¤©æ°”å·¥å…· provider"""

    async def list_runtime_tools(self, ...) -> ListToolDefsResponse:
        """å®šä¹‰å·¥å…·çš„ schema"""
        return ListToolDefsResponse(data=[
            ToolDef(
                name="get_weather",
                description="Get current weather for a city",
                input_schema={
                    "properties": {
                        "city": {"type": "string"},
                        "units": {"type": "string", "enum": ["celsius", "fahrenheit"]}
                    },
                    "required": ["city"]
                }
            )
        ])

    async def invoke_tool(self, tool_name, kwargs, ...) -> ToolInvocationResult:
        """æ‰§è¡Œå·¥å…·é€»è¾‘"""
        city = kwargs.get("city")
        units = kwargs.get("units", "celsius")

        # ç”Ÿæˆ mock å¤©æ°”æ•°æ®
        weather_data = self._get_mock_weather(city, units)

        return ToolInvocationResult(content=weather_data)
```

### å…³é”®ç‰¹æ€§

1. **è¾“å…¥éªŒè¯** - æ£€æŸ¥ city å‚æ•°ï¼ŒéªŒè¯ units
2. **é”™è¯¯å¤„ç†** - è¿”å› ToolInvocationResult å¸¦ error_code
3. **Mock æ•°æ®** - ç”Ÿæˆéšæœºæ¸©åº¦ã€æ¹¿åº¦ã€é£é€Ÿç­‰
4. **Metrics è‡ªåŠ¨è®°å½•** - é€šè¿‡ ToolRuntimeRouter çš„æ’æ¡©ä»£ç 

## ğŸ“Š æŸ¥çœ‹ Metrics

### Prometheus (http://localhost:9090)

æŸ¥è¯¢ç¤ºä¾‹ï¼š

```promql
# å¤©æ°”å·¥å…·è°ƒç”¨æ€»æ•°
llama_stack_tool_runtime_invocations_total{tool_name="get_weather"}

# æ¯ç§’è°ƒç”¨ç‡
rate(llama_stack_tool_runtime_invocations_total{tool_name="get_weather"}[1m])

# æˆåŠŸç‡
sum(rate(llama_stack_tool_runtime_invocations_total{tool_name="get_weather",status="success"}[1m]))
/
sum(rate(llama_stack_tool_runtime_invocations_total{tool_name="get_weather"}[1m]))

# P95 å»¶è¿Ÿ
histogram_quantile(0.95,
  rate(llama_stack_tool_runtime_duration_seconds_bucket{tool_name="get_weather"}[1m])
)
```

### Grafana (http://localhost:3000)

1. ç™»å½•: `admin` / `admin`
2. å¯¼èˆªåˆ°: **Dashboards â†’ Llama Stack â†’ Tool Runtime Metrics**
3. æŸ¥çœ‹é¢æ¿ï¼š
   - Tool Invocation Rate
   - Success vs Error Rate
   - Tool Latency (P50, P95, P99)
   - Invocations by Tool Group

ä½ åº”è¯¥èƒ½çœ‹åˆ° `get_weather` å·¥å…·çš„è°ƒç”¨æ•°æ®ã€‚

## ğŸ¯ é¢„æœŸè¾“å‡º

### æ§åˆ¶å°è¾“å‡º

```
==============================================================
ğŸ¦™ Llama Stack - Weather Tool Metrics Test
==============================================================

ğŸ”§ Configuration:
   Mode: Library (Direct)

âœ… OTLP Export: http://localhost:4318

ğŸ“š Initializing library mode (no server needed)
âœ… Library client initialized with weather tool

ğŸ“‹ Listing available tools...
   Found 1 tool(s):
   - get_weather: Get current weather information for a city

==============================================================
ğŸš€ Starting weather tool invocation tests
==============================================================
Duration: 120 seconds
Rate: 1.0 requests/second
Total expected requests: ~120

âœ… get_weather(Tokyo): 0.01s
   Weather in Tokyo:
âœ… get_weather(London): 0.01s
   Weather in London:
âœ… get_weather(Paris): 0.01s
   Weather in Paris:

â±ï¸  10s | Requests: 10 | Success: 10 | Failed: 0 | Rate: 100.0%
...
```

### Grafana Dashboard

åº”è¯¥çœ‹åˆ°ï¼š
- **Invocation Rate**: ~1 req/s
- **Success Rate**: ~100%
- **Latency**: P95 < 0.1s (å› ä¸ºæ˜¯ mock æ•°æ®)
- **Total Invocations**: é€æ¸å¢åŠ åˆ° ~120

## ğŸ”„ ä¸¤ç§è¿è¡Œæ¨¡å¼

### Library æ¨¡å¼ (é»˜è®¤)

```bash
export USE_SERVER_MODE="false"
python test_weather_tool.py
```

**ä¼˜ç‚¹:**
- æ— éœ€å¯åŠ¨ llama-stack server
- æ›´å¿«çš„å¯åŠ¨æ—¶é—´
- æ›´ç®€å•çš„ setup

**ç”¨é€”:**
- å¿«é€Ÿæµ‹è¯•å’Œå¼€å‘
- å•å…ƒæµ‹è¯•
- Demo å’Œæ•™å­¦

### Server æ¨¡å¼

```bash
# 1. å¯åŠ¨ server
llama stack run weather-tool-config.yaml

# 2. è¿è¡Œæµ‹è¯•
export USE_SERVER_MODE="true"
export LLAMA_STACK_URL="http://localhost:5001"
python test_weather_tool.py
```

**ä¼˜ç‚¹:**
- æ›´æ¥è¿‘ç”Ÿäº§ç¯å¢ƒ
- å¯ä»¥ä»å¤šä¸ª client è¿æ¥
- æ”¯æŒå®Œæ•´çš„ API

**ç”¨é€”:**
- ç”Ÿäº§éƒ¨ç½²
- é›†æˆæµ‹è¯•
- å¤šå®¢æˆ·ç«¯åœºæ™¯

**æ³¨æ„:** Server æ¨¡å¼éœ€è¦åœ¨ llama-stack ä¸­æ­£ç¡®æ³¨å†Œ provider (éœ€è¦ä¿®æ”¹ provider registry)

## ğŸ› ï¸ è‡ªå®šä¹‰ä½ çš„å·¥å…·

### 1. ä¿®æ”¹å·¥å…·é€»è¾‘

ç¼–è¾‘ `weather_tool/weather_provider.py`:

```python
async def invoke_tool(self, tool_name, kwargs, ...) -> ToolInvocationResult:
    # æ›¿æ¢ mock æ•°æ®ä¸ºçœŸå® API è°ƒç”¨
    city = kwargs.get("city")

    # ä¾‹å¦‚ï¼šè°ƒç”¨ OpenWeatherMap API
    api_key = os.environ.get("OPENWEATHER_API_KEY")
    url = f"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}"

    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        data = response.json()

    return ToolInvocationResult(content=format_weather(data))
```

### 2. æ·»åŠ æ–°å·¥å…·

åœ¨ `list_runtime_tools` ä¸­æ·»åŠ ï¼š

```python
return ListToolDefsResponse(data=[
    ToolDef(name="get_weather", ...),
    ToolDef(
        name="get_forecast",
        description="Get 5-day weather forecast",
        input_schema={...}
    ),
])
```

åœ¨ `invoke_tool` ä¸­å¤„ç†ï¼š

```python
if tool_name == "get_weather":
    return await self._get_current_weather(kwargs)
elif tool_name == "get_forecast":
    return await self._get_forecast(kwargs)
```

### 3. ä¿®æ”¹æµ‹è¯•å‚æ•°

ç¼–è¾‘ `test_weather_tool.py` çš„ `main()` å‡½æ•°ï¼š

```python
await tester.run_weather_tests(
    duration_seconds=300,  # 5 åˆ†é’Ÿ
    requests_per_second=2.0,  # 2 req/s
)
```

## ğŸ› æ•…éšœæ’é™¤

### å¯¼å…¥é”™è¯¯: "No module named 'examples'"

```bash
# ç¡®ä¿ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
cd /path/to/llama-stack
python examples/metrics-demo/test_weather_tool.py
```

æˆ–è®¾ç½® PYTHONPATH:

```bash
export PYTHONPATH=/path/to/llama-stack:$PYTHONPATH
```

### OTLP Collector è¿æ¥è¢«æ‹’ç»

```bash
# æ£€æŸ¥ Docker stack
docker-compose ps

# é‡å¯ collector
docker-compose restart otel-collector

# æ£€æŸ¥æ—¥å¿—
docker-compose logs otel-collector
```

### Grafana ä¸­æ²¡æœ‰æ•°æ®

1. æ£€æŸ¥æ—¶é—´èŒƒå›´: è®¾ç½®ä¸º "Last 15 minutes"
2. å¯ç”¨è‡ªåŠ¨åˆ·æ–°: 5s
3. æ£€æŸ¥ Prometheus æ˜¯å¦æœ‰æ•°æ®: http://localhost:9090
4. éªŒè¯ metric åç§°: `llama_stack_tool_runtime_invocations_total`

## ğŸ“š æ‰©å±•é˜…è¯»

- [llama-stack Tool Runtime API](https://llama-stack.com/docs/tool-runtime)
- [OpenTelemetry Metrics](https://opentelemetry.io/docs/concepts/signals/metrics/)
- [Prometheus PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Grafana Dashboards](https://grafana.com/docs/grafana/latest/dashboards/)

## ğŸ“ å­¦ä¹ è¦ç‚¹

é€šè¿‡è¿™ä¸ªç¤ºä¾‹ï¼Œä½ å­¦ä¼šäº†ï¼š

1. âœ… å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ llama-stack tool provider
2. âœ… å¦‚ä½•å®šä¹‰å·¥å…·çš„ input/output schema
3. âœ… å¦‚ä½•å®ç° `invoke_tool` é€»è¾‘
4. âœ… å¦‚ä½•ä½¿ç”¨ library æ¨¡å¼æµ‹è¯•å·¥å…·
5. âœ… å¦‚ä½•è‡ªåŠ¨è®°å½•å’Œå¯¼å‡º metrics
6. âœ… å¦‚ä½•åœ¨ Prometheus å’Œ Grafana ä¸­å¯è§†åŒ– metrics

## ğŸš€ ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å¯ä»¥ï¼š

1. **é›†æˆçœŸå® API** - æ›¿æ¢ mock æ•°æ®ä¸ºçœŸå®çš„å¤©æ°” API
2. **æ·»åŠ æ›´å¤šå·¥å…·** - åˆ›å»ºå·¥å…·ç»„ï¼Œå¦‚æœç´¢ã€è®¡ç®—ã€æ•°æ®åº“æŸ¥è¯¢
3. **éƒ¨ç½²åˆ°ç”Ÿäº§** - ä½¿ç”¨ server æ¨¡å¼ï¼Œé…ç½®è´Ÿè½½å‡è¡¡
4. **ç›‘æ§å‘Šè­¦** - åœ¨ Grafana ä¸­è®¾ç½®å‘Šè­¦è§„åˆ™
5. **ä¼˜åŒ–æ€§èƒ½** - ä½¿ç”¨ metrics å‘ç°ç“¶é¢ˆå¹¶ä¼˜åŒ–

Happy coding! ğŸ¦™ğŸ“Š

# Llama Stack è‡ªå®šä¹‰å¤©æ°”å·¥å…·å®Œæ•´æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è®°å½•äº†å¦‚ä½•åˆ›å»ºã€æ³¨å†Œå’Œç›‘æ§ä¸€ä¸ªè‡ªå®šä¹‰çš„ llama-stack toolï¼ŒåŒ…æ‹¬é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚

## ç›®å½•

1. [é¡¹ç›®èƒŒæ™¯](#é¡¹ç›®èƒŒæ™¯)
2. [åˆ›å»ºè‡ªå®šä¹‰ Tool Provider](#åˆ›å»ºè‡ªå®šä¹‰-tool-provider)
3. [æ³¨å†Œ Provider åˆ° Registry](#æ³¨å†Œ-provider-åˆ°-registry)
4. [é…ç½®å’Œå¯åŠ¨ Server](#é…ç½®å’Œå¯åŠ¨-server)
5. [é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ](#é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ)
6. [Metrics ç”Ÿæˆæœºåˆ¶](#metrics-ç”Ÿæˆæœºåˆ¶)
7. [å®Œæ•´ Workflow](#å®Œæ•´-workflow)
8. [éªŒè¯å’Œæµ‹è¯•](#éªŒè¯å’Œæµ‹è¯•)

---

## é¡¹ç›®èƒŒæ™¯

### ç›®æ ‡

åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„å¤©æ°”å·¥å…·ï¼Œæ³¨å†Œåˆ° llama-stack serverï¼Œè®©å®¢æˆ·ç«¯å¯ä»¥ï¼š
- é€šè¿‡ `llama-stack-client` CLI æŸ¥è¯¢åˆ°å·¥å…·
- é€šè¿‡ Python SDK è°ƒç”¨å·¥å…·
- è‡ªåŠ¨è®°å½•å·¥å…·è°ƒç”¨çš„ OpenTelemetry metrics
- åœ¨ Prometheus å’Œ Grafana ä¸­å¯è§†åŒ–ç›‘æ§

### éœ€æ±‚

- å·¥å…·åç§°: `get_weather`
- å·¥å…·ç»„: `weather`
- Provider ID: `weather-runtime`
- Provider ç±»å‹: `inline::weather`
- Metrics: è‡ªåŠ¨è®°å½•è°ƒç”¨æ¬¡æ•°ã€å»¶è¿Ÿã€æˆåŠŸç‡ç­‰

---

## åˆ›å»ºè‡ªå®šä¹‰ Tool Provider

### æ–‡ä»¶ç»“æ„

```
src/llama_stack/providers/inline/tool_runtime/weather/
â”œâ”€â”€ __init__.py          # Provider å¯¼å‡ºå‡½æ•°
â”œâ”€â”€ config.py            # é…ç½®ç±»å®šä¹‰
â””â”€â”€ weather.py           # å·¥å…·å®ç°é€»è¾‘
```

### 1. åˆ›å»ºé…ç½®ç±» (config.py)

```python
# src/llama_stack/providers/inline/tool_runtime/weather/config.py

from typing import Any
from pydantic import BaseModel


class WeatherToolRuntimeConfig(BaseModel):
    """Configuration for the weather tool runtime provider."""

    @classmethod
    def sample_run_config(cls, __distro_dir__: str, **kwargs: Any) -> dict[str, Any]:
        return {}
```

**è¯´æ˜**ï¼š
- ä½¿ç”¨ Pydantic BaseModel å®šä¹‰é…ç½®
- `sample_run_config` æ–¹æ³•è¿”å›é»˜è®¤é…ç½®
- å¯ä»¥æ·»åŠ é…ç½®å‚æ•°ï¼ˆå¦‚ API keyã€é»˜è®¤å•ä½ç­‰ï¼‰

### 2. å®ç° Provider ç±» (weather.py)

```python
# src/llama_stack/providers/inline/tool_runtime/weather/weather.py

import random
from typing import Any

from llama_stack_api import (
    URL,
    ListToolDefsResponse,
    ToolDef,
    ToolGroup,
    ToolGroupsProtocolPrivate,
    ToolInvocationResult,
    ToolRuntime,
)

from .config import WeatherToolRuntimeConfig


class WeatherToolRuntimeImpl(ToolGroupsProtocolPrivate, ToolRuntime):
    """Weather tool runtime implementation."""

    def __init__(self, config: WeatherToolRuntimeConfig):
        self.config = config
        self.tool_name = "get_weather"
        self.toolgroup_id = "weather"

    async def initialize(self):
        """Initialize the weather tool provider."""
        pass

    async def shutdown(self):
        """Shutdown the weather tool provider."""
        pass

    async def register_toolgroup(self, toolgroup: ToolGroup) -> None:
        """Register a tool group (no-op for this provider)."""
        pass

    async def unregister_toolgroup(self, toolgroup_id: str) -> None:
        """Unregister a tool group (no-op for this provider)."""
        pass

    async def list_runtime_tools(
        self,
        tool_group_id: str | None = None,
        mcp_endpoint: URL | None = None,
        authorization: str | None = None,
    ) -> ListToolDefsResponse:
        """List available tools in this provider."""
        return ListToolDefsResponse(
            data=[
                ToolDef(
                    toolgroup_id=self.toolgroup_id,
                    name=self.tool_name,
                    description="Get current weather information for a city",
                    input_schema={
                        "type": "object",
                        "properties": {
                            "city": {
                                "type": "string",
                                "description": "The city name to get weather for",
                            },
                            "units": {
                                "type": "string",
                                "enum": ["celsius", "fahrenheit"],
                                "description": "Temperature unit. Defaults to celsius.",
                            },
                        },
                        "required": ["city"],
                    },
                )
            ]
        )

    async def invoke_tool(
        self, tool_name: str, kwargs: dict[str, Any], authorization: str | None = None
    ) -> ToolInvocationResult:
        """Invoke the weather tool."""
        if tool_name != self.tool_name:
            return ToolInvocationResult(
                error_message=f"Unknown tool: {tool_name}",
                error_code=404,
            )

        city = kwargs.get("city")
        if not city:
            return ToolInvocationResult(
                error_message="Missing required parameter: city",
                error_code=400,
            )

        units = kwargs.get("units", "celsius")

        # Generate mock weather data
        weather_data = self._get_mock_weather(city, units)

        return ToolInvocationResult(
            content=weather_data,
            metadata={"provider": "weather-tool-inline"},
        )

    def _get_mock_weather(self, city: str, units: str) -> str:
        """Generate mock weather data."""
        conditions = ["Sunny", "Cloudy", "Rainy", "Snowy"]
        condition = random.choice(conditions)

        if units == "celsius":
            temp = round(random.uniform(-5, 35), 1)
            temp_unit = "Â°C"
        else:
            temp = round(random.uniform(23, 95), 1)
            temp_unit = "Â°F"

        humidity = random.randint(30, 95)
        wind_speed = round(random.uniform(0, 30), 1)

        return f"""Weather in {city}:
Temperature: {temp}{temp_unit}
Condition: {condition}
Humidity: {humidity}%
Wind Speed: {wind_speed} km/h"""
```

**å…³é”®ç‚¹**ï¼š

1. **ç»§æ‰¿å¿…éœ€çš„åè®®**ï¼š
   - `ToolRuntime`: å·¥å…·è¿è¡Œæ—¶åè®®
   - `ToolGroupsProtocolPrivate`: å·¥å…·ç»„ç®¡ç†åè®®

2. **å®ç°å¿…éœ€æ–¹æ³•**ï¼š
   - `initialize()` / `shutdown()`: ç”Ÿå‘½å‘¨æœŸç®¡ç†
   - `list_runtime_tools()`: åˆ—å‡ºå¯ç”¨å·¥å…·åŠå…¶ schema
   - `invoke_tool()`: æ‰§è¡Œå·¥å…·é€»è¾‘
   - `register_toolgroup()` / `unregister_toolgroup()`: å·¥å…·ç»„ç®¡ç†

3. **å®šä¹‰å·¥å…· Schema**ï¼š
   - `input_schema`: JSON Schema å®šä¹‰è¾“å…¥å‚æ•°
   - `output_schema`: (å¯é€‰) å®šä¹‰è¾“å‡ºæ ¼å¼
   - `description`: å·¥å…·æè¿°

4. **é”™è¯¯å¤„ç†**ï¼š
   - å‚æ•°éªŒè¯
   - è¿”å› `ToolInvocationResult` å¸¦ `error_message` å’Œ `error_code`

### 3. åˆ›å»º Provider å¯¼å‡º (__init__.py)

```python
# src/llama_stack/providers/inline/tool_runtime/weather/__init__.py

from typing import Any
from llama_stack_api import Api
from .config import WeatherToolRuntimeConfig


async def get_provider_impl(config: WeatherToolRuntimeConfig, deps: dict[Api, Any]):
    """Factory function to create the provider implementation."""
    from .weather import WeatherToolRuntimeImpl

    impl = WeatherToolRuntimeImpl(config)
    await impl.initialize()
    return impl
```

**è¯´æ˜**ï¼š
- `get_provider_impl` æ˜¯æ ‡å‡†çš„ provider å·¥å‚å‡½æ•°
- æ¥æ”¶é…ç½®å’Œä¾èµ–
- è¿”å›åˆå§‹åŒ–åçš„ provider å®ä¾‹

---

## æ³¨å†Œ Provider åˆ° Registry

### ä¿®æ”¹ Tool Runtime Registry

```python
# src/llama_stack/providers/registry/tool_runtime.py

from llama_stack_api import Api, InlineProviderSpec


def available_providers() -> list[ProviderSpec]:
    return [
        # ... å…¶ä»– providers ...

        # æ·»åŠ  Weather Provider
        InlineProviderSpec(
            api=Api.tool_runtime,
            provider_type="inline::weather",
            pip_packages=[],  # æ— é¢å¤–ä¾èµ–
            module="llama_stack.providers.inline.tool_runtime.weather",
            config_class="llama_stack.providers.inline.tool_runtime.weather.config.WeatherToolRuntimeConfig",
            api_dependencies=[],  # æ— ä¾èµ–å…¶ä»– API
            description="Weather tool for getting current weather information (mock).",
        ),
    ]
```

**å…³é”®å­—æ®µ**ï¼š

- `api`: API ç±»å‹ (`Api.tool_runtime`)
- `provider_type`: Provider å”¯ä¸€æ ‡è¯†ç¬¦ (`inline::weather`)
- `module`: Python æ¨¡å—è·¯å¾„
- `config_class`: é…ç½®ç±»çš„å®Œæ•´è·¯å¾„
- `pip_packages`: éœ€è¦çš„ Python åŒ…ï¼ˆå¯é€‰ï¼‰
- `api_dependencies`: ä¾èµ–çš„å…¶ä»– APIï¼ˆå¯é€‰ï¼‰

### é‡æ–°å®‰è£… llama-stack

```bash
pip install -e .
```

**ä½œç”¨**ï¼š
- é‡æ–°å®‰è£…ç¡®ä¿ registry çš„ä¿®æ”¹è¢«åŠ è½½
- Provider ä»£ç å˜æ›´ä¼šè¢«è¯†åˆ«

---

## é…ç½®å’Œå¯åŠ¨ Server

### 1. åˆ›å»ºé…ç½®æ–‡ä»¶

```yaml
# weather-stack-config.yaml

version: 2

# ä½¿ç”¨çš„åŸºç¡€åˆ†å‘åŒ…
image_name: llamastack/distribution-together

providers:
  # é…ç½® Tool Runtime Provider
  tool_runtime:
    - provider_id: weather-runtime      # Provider å®ä¾‹ ID
      provider_type: inline::weather    # æ³¨å†Œçš„ provider ç±»å‹
      config: {}                        # Provider é…ç½®ï¼ˆç©ºè¡¨ç¤ºä½¿ç”¨é»˜è®¤ï¼‰

# æ³¨å†Œ Tool Groups
tool_groups:
  - toolgroup_id: weather               # Tool group ID
    provider_id: weather-runtime        # å…³è”çš„ provider
```

### 2. å¯åŠ¨ Serverï¼ˆå¸¦ Metricsï¼‰

**é‡è¦**ï¼šéœ€è¦è®¾ç½® OpenTelemetry ç¯å¢ƒå˜é‡æ‰èƒ½å¯¼å‡º metricsï¼

```bash
# è®¾ç½® OTEL ç¯å¢ƒå˜é‡
export OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4318"
export OTEL_EXPORTER_OTLP_PROTOCOL="http/protobuf"
export OTEL_SERVICE_NAME="llama-stack-server"
export OTEL_METRIC_EXPORT_INTERVAL="5000"  # 5ç§’å¯¼å‡ºä¸€æ¬¡

# å¯åŠ¨ server
llama stack run weather-stack-config.yaml --port 8321
```

**æˆ–ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬**ï¼š

```bash
./start_weather_with_metrics.sh
```

### 3. æ³¨å†Œ Tool Group

**é—®é¢˜**ï¼šé…ç½®æ–‡ä»¶ä¸­çš„ `tool_groups` æœ‰æ—¶ä¸ä¼šè‡ªåŠ¨æ³¨å†Œã€‚

**è§£å†³**ï¼šé€šè¿‡ API æ‰‹åŠ¨æ³¨å†Œä¸€æ¬¡ï¼š

```bash
curl -X POST http://localhost:8321/v1/toolgroups \
  -H "Content-Type: application/json" \
  -d '{
    "toolgroup_id": "weather",
    "provider_id": "weather-runtime"
  }'
```

---

## é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜ 1: Tool Group ä¸æ˜¾ç¤º

**ç°è±¡**ï¼š
```bash
llama-stack-client toolgroups list
# è¾“å‡º: ç©ºè¡¨æ ¼ï¼Œæ²¡æœ‰æ•°æ®
```

**åŸå› **ï¼š
- é…ç½®æ–‡ä»¶ä¸­çš„ `tool_groups` éƒ¨åˆ†åœ¨æŸäº›æƒ…å†µä¸‹ä¸ä¼šè‡ªåŠ¨æ³¨å†Œ
- å¯èƒ½æ˜¯ server å¯åŠ¨é¡ºåºæˆ–é…ç½®åŠ è½½çš„é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š

æ‰‹åŠ¨é€šè¿‡ API æ³¨å†Œï¼š

```bash
curl -X POST http://localhost:8321/v1/toolgroups \
  -H "Content-Type: application/json" \
  -d '{
    "toolgroup_id": "weather",
    "provider_id": "weather-runtime"
  }'
```

**éªŒè¯**ï¼š

```bash
llama-stack-client toolgroups list
# åº”è¯¥æ˜¾ç¤º:
# â”ƒ identifier â”ƒ provider_id     â”ƒ
# â”ƒ weather    â”ƒ weather-runtime â”ƒ
```

### é—®é¢˜ 2: Python Client API ä¸åŒ¹é…

**ç°è±¡**ï¼š

```python
client = LlamaStackClient(base_url="http://localhost:8321")
result = client.tool_groups.list()  # AttributeError: no attribute 'tool_groups'
```

**åŸå› **ï¼š
- Python SDK çš„ API å‘½åä¸ CLI ä¸åŒ
- åº”è¯¥ä½¿ç”¨ `toolgroups` è€Œä¸æ˜¯ `tool_groups`

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# âœ… æ­£ç¡®çš„ API
result = client.toolgroups.list()  # æ³¨æ„æ˜¯ toolgroupsï¼ˆå…¨å°å†™ï¼‰

# å¤„ç†è¿”å›å€¼ï¼ˆå¯èƒ½æ˜¯ list æˆ–å¸¦ .data å±æ€§çš„å¯¹è±¡ï¼‰
tool_groups = result.data if hasattr(result, 'data') else result
```

### é—®é¢˜ 3: Prometheus æ²¡æœ‰ Metrics

**ç°è±¡**ï¼š

è¿è¡Œ `python test_weather_client.py` åï¼ŒPrometheus æŸ¥è¯¢è¿”å›ç©ºç»“æœï¼š

```bash
curl 'http://localhost:9090/api/v1/query?query=llama_stack_tool_runtime_invocations_total'
# {"data": {"result": []}}
```

**åŸå› **ï¼š

Server å¯åŠ¨æ—¶**æ²¡æœ‰è®¾ç½® OpenTelemetry ç¯å¢ƒå˜é‡**ï¼Œå¯¼è‡´ï¼š

1. `src/llama_stack/telemetry/__init__.py` ä¸­çš„ `setup_telemetry()` æ£€æµ‹åˆ° `OTEL_EXPORTER_OTLP_ENDPOINT` æœªè®¾ç½®
2. è·³è¿‡äº† OTLP exporter é…ç½®
3. è™½ç„¶ä»£ç ä¸­æœ‰ metrics æ’æ¡©ï¼Œä½† metrics æ²¡æœ‰å¯¼å‡º

**ä»£ç é€»è¾‘**ï¼š

```python
# src/llama_stack/telemetry/__init__.py

def setup_telemetry():
    otlp_endpoint = os.environ.get("OTEL_EXPORTER_OTLP_ENDPOINT")

    if not otlp_endpoint:
        logger.debug("OTEL_EXPORTER_OTLP_ENDPOINT not set, metrics will not be exported")
        return  # âš ï¸ è¿™é‡Œå°±è¿”å›äº†ï¼Œä¸ä¼šé…ç½® exporter

    # ... é…ç½® OTLP exporter ...
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

å¯åŠ¨ server å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
export OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4318"
export OTEL_EXPORTER_OTLP_PROTOCOL="http/protobuf"
export OTEL_SERVICE_NAME="llama-stack-server"
export OTEL_METRIC_EXPORT_INTERVAL="5000"

llama stack run weather-stack-config.yaml --port 8321
```

**éªŒè¯ Telemetry å·²åˆå§‹åŒ–**ï¼š

æ£€æŸ¥ server æ—¥å¿—ï¼š

```bash
grep -i "OpenTelemetry metrics exporter configured" /tmp/llama-stack-weather-metrics.log
# åº”è¯¥è¾“å‡º: INFO ... OpenTelemetry metrics exporter configured: http://localhost:4318
```

### é—®é¢˜ 4: OTLP Collector ä¸å¥åº·

**ç°è±¡**ï¼š

```bash
docker-compose ps
# otel-collector æ˜¾ç¤º unhealthy
```

**åŸå› **ï¼š

ä¹‹å‰çš„ `otel-collector-config.yaml` é…ç½®é”™è¯¯ï¼ŒåŒæ—¶ä½¿ç”¨äº† `loglevel` å’Œ `verbosity`ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

```yaml
# otel-collector-config.yaml

exporters:
  logging:
    # loglevel: info  âŒ åˆ é™¤è¿™è¡Œ
    verbosity: detailed  # âœ… åªä¿ç•™ verbosity
```

---

## Metrics ç”Ÿæˆæœºåˆ¶

### Metrics æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Client è°ƒç”¨å·¥å…·                                          â”‚
â”‚    client.tool_runtime.invoke_tool(                         â”‚
â”‚        tool_name="get_weather",                             â”‚
â”‚        kwargs={"city": "Tokyo"}                             â”‚
â”‚    )                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ToolRuntimeRouter.invoke_tool (å·²æ’æ¡©!)                  â”‚
â”‚    - src/llama_stack/core/routers/tool_runtime.py          â”‚
â”‚                                                             â”‚
â”‚    async def invoke_tool(self, tool_name, kwargs, ...):    â”‚
â”‚        start_time = time.perf_counter()  # å¼€å§‹è®¡æ—¶        â”‚
â”‚                                                             â”‚
â”‚        try:                                                 â”‚
â”‚            # è°ƒç”¨å®é™…çš„ provider                            â”‚
â”‚            result = await provider.invoke_tool(...)        â”‚
â”‚                                                             â”‚
â”‚            # è®°å½•æˆåŠŸ metrics                               â”‚
â”‚            duration = time.perf_counter() - start_time     â”‚
â”‚            tool_invocations_total.add(1, {                 â”‚
â”‚                "tool_name": "get_weather",                 â”‚
â”‚                "tool_group": "weather",                    â”‚
â”‚                "provider": "weather-runtime",              â”‚
â”‚                "status": "success"                         â”‚
â”‚            })                                              â”‚
â”‚            tool_duration.record(duration, {...})           â”‚
â”‚                                                             â”‚
â”‚        except Exception:                                    â”‚
â”‚            # è®°å½•å¤±è´¥ metrics                               â”‚
â”‚            tool_invocations_total.add(1, {                 â”‚
â”‚                "status": "error"                           â”‚
â”‚            })                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. WeatherToolRuntimeImpl.invoke_tool                       â”‚
â”‚    - æ‰§è¡Œå®é™…çš„å¤©æ°”æŸ¥è¯¢é€»è¾‘                                 â”‚
â”‚    - è¿”å› ToolInvocationResult                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. OpenTelemetry MeterProvider                              â”‚
â”‚    - æ”¶é›† metrics æ•°æ®                                      â”‚
â”‚    - æ¯ 5 ç§’ (OTEL_METRIC_EXPORT_INTERVAL) å¯¼å‡ºä¸€æ¬¡        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. OTLP Exporter                                            â”‚
â”‚    - é€šè¿‡ HTTP å‘é€åˆ° http://localhost:4318/v1/metrics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. OTLP Collector (Docker)                                  â”‚
â”‚    - æ¥æ”¶ OTLP æ ¼å¼çš„ metrics                               â”‚
â”‚    - æš´éœ² Prometheus æ ¼å¼åœ¨ port 8889                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Prometheus                                               â”‚
â”‚    - æ¯ 15 ç§’æŠ“å– otel-collector:8889/metrics              â”‚
â”‚    - å­˜å‚¨æ—¶é—´åºåˆ—æ•°æ®                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Grafana Dashboard                                        â”‚
â”‚    - æŸ¥è¯¢ Prometheus                                        â”‚
â”‚    - å¯è§†åŒ– metrics                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Metrics æ’æ¡©ä»£ç 

**ä½ç½®**: `src/llama_stack/core/routers/tool_runtime.py`

```python
from llama_stack.telemetry.metrics import (
    tool_invocations_total,
    tool_duration,
    create_tool_metric_attributes,
)

class ToolRuntimeRouter:
    async def invoke_tool(
        self,
        tool_name: str,
        kwargs: dict[str, Any],
        authorization: str | None = None
    ) -> Any:
        start_time = time.perf_counter()
        metric_attrs = None

        try:
            # è·å– provider å’Œ tool group ä¿¡æ¯
            provider = await self.routing_table.get_provider_impl(tool_name)
            tool_group = self.routing_table.tool_to_toolgroup.get(tool_name)

            # åˆ›å»º metric å±æ€§
            metric_attrs = create_tool_metric_attributes(
                tool_group=tool_group,
                tool_name=tool_name,
                provider=getattr(provider, "__provider_id__", None),
            )

            # è°ƒç”¨å®é™…çš„ provider
            result = await provider.invoke_tool(
                tool_name=tool_name,
                kwargs=kwargs,
                authorization=authorization,
            )

            # è®°å½•æˆåŠŸ metrics
            duration = time.perf_counter() - start_time
            success_attrs = {**metric_attrs, "status": "success"}

            tool_invocations_total.add(1, success_attrs)  # Counter +1
            tool_duration.record(duration, metric_attrs)  # Histogram è®°å½•å»¶è¿Ÿ

            return result

        except Exception as e:
            # è®°å½•å¤±è´¥ metrics
            duration = time.perf_counter() - start_time
            error_attrs = {**metric_attrs, "status": "error"} if metric_attrs else {
                "tool_name": tool_name,
                "status": "error",
            }

            tool_invocations_total.add(1, error_attrs)
            tool_duration.record(duration, error_attrs)

            raise
```

### Metrics å®šä¹‰

**ä½ç½®**: `src/llama_stack/telemetry/metrics.py`

```python
from opentelemetry import metrics
from llama_stack.telemetry.constants import (
    TOOL_INVOCATIONS_TOTAL,
    TOOL_DURATION,
)

# è·å– Meter
meter = metrics.get_meter("llama_stack.tool_runtime", version="1.0.0")

# å®šä¹‰ Counter: è°ƒç”¨æ€»æ•°
tool_invocations_total = meter.create_counter(
    name=TOOL_INVOCATIONS_TOTAL,  # "llama_stack.tool_runtime.invocations_total"
    description="Total number of tool invocations processed by the runtime",
    unit="1",
)

# å®šä¹‰ Histogram: è°ƒç”¨å»¶è¿Ÿ
tool_duration = meter.create_histogram(
    name=TOOL_DURATION,  # "llama_stack.tool_runtime.duration_seconds"
    description="Duration of tool invocations from start to completion",
    unit="s",
)
```

### Metric å±æ€§ (Labels)

æ¯ä¸ª metric éƒ½ä¼šå¸¦ä¸Šè¿™äº›æ ‡ç­¾ç”¨äºè¿‡æ»¤å’Œèšåˆï¼š

```python
{
    "tool_name": "get_weather",        # å·¥å…·åç§°
    "tool_group": "weather",           # å·¥å…·ç»„
    "provider": "weather-runtime",     # Provider ID
    "status": "success" | "error",     # è°ƒç”¨çŠ¶æ€
    "service_name": "llama-stack-server",  # æœåŠ¡å
}
```

### ç”Ÿæˆçš„ Metrics

åœ¨ Prometheus ä¸­ä¼šçœ‹åˆ°ï¼š

```promql
# Counter: è°ƒç”¨æ¬¡æ•°
llama_stack_tool_runtime_invocations_total{
    tool_name="get_weather",
    tool_group="weather",
    provider="weather-runtime",
    status="success"
} = 3

# Histogram: å»¶è¿Ÿåˆ†å¸ƒ
llama_stack_tool_runtime_duration_seconds_bucket{
    tool_name="get_weather",
    le="0.005"  # <= 5ms
} = 0

llama_stack_tool_runtime_duration_seconds_bucket{
    tool_name="get_weather",
    le="0.01"   # <= 10ms
} = 3  # æ‰€æœ‰3æ¬¡è°ƒç”¨éƒ½åœ¨ 10ms å†…

llama_stack_tool_runtime_duration_seconds_sum{
    tool_name="get_weather"
} = 0.025  # æ€»è€—æ—¶

llama_stack_tool_runtime_duration_seconds_count{
    tool_name="get_weather"
} = 3  # æ€»æ¬¡æ•°
```

---

## å®Œæ•´ Workflow

### å¼€å‘ Workflow

```
1. åˆ›å»º Provider æ–‡ä»¶
   â”œâ”€â”€ config.py         (é…ç½®ç±»)
   â”œâ”€â”€ weather.py        (å®ç°é€»è¾‘)
   â””â”€â”€ __init__.py       (å¯¼å‡ºå‡½æ•°)

2. æ³¨å†Œåˆ° Registry
   â””â”€â”€ ä¿®æ”¹ src/llama_stack/providers/registry/tool_runtime.py

3. é‡æ–°å®‰è£…
   â””â”€â”€ pip install -e .

4. åˆ›å»ºé…ç½®æ–‡ä»¶
   â””â”€â”€ weather-stack-config.yaml

5. å¯åŠ¨ Server (å¸¦ OTEL ç¯å¢ƒå˜é‡)
   â””â”€â”€ è®¾ç½® OTEL_* ç¯å¢ƒå˜é‡
   â””â”€â”€ llama stack run weather-stack-config.yaml

6. æ³¨å†Œ Tool Group
   â””â”€â”€ curl POST /v1/toolgroups

7. æµ‹è¯•è°ƒç”¨
   â””â”€â”€ python test_weather_client.py

8. æŸ¥çœ‹ Metrics
   â””â”€â”€ Prometheus: http://localhost:9090
   â””â”€â”€ Grafana: http://localhost:3000
```

### è¿è¡Œæ—¶ Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç”¨æˆ·æ“ä½œ                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. å¯åŠ¨ç›‘æ§æ ˆ                                               â”‚
â”‚    cd examples/metrics-demo                                 â”‚
â”‚    docker-compose up -d                                     â”‚
â”‚                                                             â”‚
â”‚    å¯åŠ¨æœåŠ¡:                                                â”‚
â”‚    â”œâ”€â”€ OTLP Collector (port 4318, 8889)                    â”‚
â”‚    â”œâ”€â”€ Prometheus (port 9090)                              â”‚
â”‚    â””â”€â”€ Grafana (port 3000)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. é…ç½®ç¯å¢ƒå˜é‡                                             â”‚
â”‚    export OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4318" â”‚
â”‚    export OTEL_EXPORTER_OTLP_PROTOCOL="http/protobuf"      â”‚
â”‚    export OTEL_SERVICE_NAME="llama-stack-server"           â”‚
â”‚    export OTEL_METRIC_EXPORT_INTERVAL="5000"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. å¯åŠ¨ Llama Stack Server                                  â”‚
â”‚    llama stack run weather-stack-config.yaml --port 8321   â”‚
â”‚                                                             â”‚
â”‚    Server åˆå§‹åŒ–:                                           â”‚
â”‚    â”œâ”€â”€ åŠ è½½é…ç½®æ–‡ä»¶                                         â”‚
â”‚    â”œâ”€â”€ æ³¨å†Œ providers (inline::weather)                    â”‚
â”‚    â”œâ”€â”€ åˆå§‹åŒ– telemetry (setup_telemetry())                â”‚
â”‚    â”‚   â””â”€â”€ é…ç½® OTLP exporter â†’ localhost:4318            â”‚
â”‚    â””â”€â”€ å¯åŠ¨ HTTP server (port 8321)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. æ³¨å†Œ Tool Group                                          â”‚
â”‚    curl POST http://localhost:8321/v1/toolgroups           â”‚
â”‚                                                             â”‚
â”‚    æ³¨å†Œä¿¡æ¯:                                                â”‚
â”‚    â”œâ”€â”€ toolgroup_id: "weather"                             â”‚
â”‚    â””â”€â”€ provider_id: "weather-runtime"                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Client è°ƒç”¨å·¥å…·                                          â”‚
â”‚    client = LlamaStackClient("http://localhost:8321")      â”‚
â”‚    result = client.tool_runtime.invoke_tool(               â”‚
â”‚        tool_name="get_weather",                            â”‚
â”‚        kwargs={"city": "Tokyo", "units": "celsius"}        â”‚
â”‚    )                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Server å¤„ç†è¯·æ±‚                                          â”‚
â”‚                                                             â”‚
â”‚    HTTP Request                                             â”‚
â”‚    POST /v1/tool-runtime/invoke                            â”‚
â”‚    Body: {                                                 â”‚
â”‚        "tool_name": "get_weather",                         â”‚
â”‚        "kwargs": {"city": "Tokyo"}                         â”‚
â”‚    }                                                       â”‚
â”‚         â†“                                                  â”‚
â”‚    ToolRuntimeRouter.invoke_tool()                         â”‚
â”‚         â”œâ”€â”€ start_time = perf_counter()                   â”‚
â”‚         â”œâ”€â”€ æŸ¥æ‰¾ provider (weather-runtime)                â”‚
â”‚         â”œâ”€â”€ æŸ¥æ‰¾ tool_group (weather)                      â”‚
â”‚         â”œâ”€â”€ åˆ›å»º metric_attrs                              â”‚
â”‚         â”‚   {                                              â”‚
â”‚         â”‚       "tool_name": "get_weather",               â”‚
â”‚         â”‚       "tool_group": "weather",                  â”‚
â”‚         â”‚       "provider": "weather-runtime"             â”‚
â”‚         â”‚   }                                              â”‚
â”‚         â†“                                                  â”‚
â”‚    WeatherToolRuntimeImpl.invoke_tool()                    â”‚
â”‚         â”œâ”€â”€ éªŒè¯å‚æ•° (city, units)                         â”‚
â”‚         â”œâ”€â”€ ç”Ÿæˆ mock å¤©æ°”æ•°æ®                             â”‚
â”‚         â””â”€â”€ è¿”å› ToolInvocationResult                      â”‚
â”‚         â†“                                                  â”‚
â”‚    è¿”å›åˆ° ToolRuntimeRouter                                 â”‚
â”‚         â”œâ”€â”€ duration = perf_counter() - start_time        â”‚
â”‚         â”œâ”€â”€ tool_invocations_total.add(1, {               â”‚
â”‚         â”‚       ...metric_attrs,                          â”‚
â”‚         â”‚       "status": "success"                       â”‚
â”‚         â”‚   })                                            â”‚
â”‚         â””â”€â”€ tool_duration.record(duration, metric_attrs)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Metrics å¯¼å‡º (æ¯ 5 ç§’ä¸€æ¬¡)                               â”‚
â”‚                                                             â”‚
â”‚    OpenTelemetry MeterProvider                              â”‚
â”‚         â†“                                                  â”‚
â”‚    æ”¶é›†æ‰€æœ‰ metrics æ•°æ®                                    â”‚
â”‚         â†“                                                  â”‚
â”‚    OTLPMetricExporter                                       â”‚
â”‚         â†“                                                  â”‚
â”‚    HTTP POST http://localhost:4318/v1/metrics              â”‚
â”‚    (OTLP Protocol Buffer æ ¼å¼)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. OTLP Collector æ¥æ”¶å’Œè½¬æ¢                                â”‚
â”‚                                                             â”‚
â”‚    OTLP Receiver (port 4318)                                â”‚
â”‚         â†“                                                  â”‚
â”‚    è§£æ OTLP æ ¼å¼                                           â”‚
â”‚         â†“                                                  â”‚
â”‚    Prometheus Exporter                                      â”‚
â”‚         â†“                                                  â”‚
â”‚    æš´éœ² Prometheus æ ¼å¼ (port 8889/metrics)                â”‚
â”‚                                                             â”‚
â”‚    æ ¼å¼ç¤ºä¾‹:                                                â”‚
â”‚    llama_stack_tool_runtime_invocations_total{             â”‚
â”‚        tool_name="get_weather",                            â”‚
â”‚        status="success"                                    â”‚
â”‚    } 1                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. Prometheus æŠ“å– (æ¯ 15 ç§’)                               â”‚
â”‚                                                             â”‚
â”‚    Scrape Job: otel-collector                              â”‚
â”‚    Target: otel-collector:8889                             â”‚
â”‚         â†“                                                  â”‚
â”‚    GET http://otel-collector:8889/metrics                  â”‚
â”‚         â†“                                                  â”‚
â”‚    è§£æ Prometheus æ–‡æœ¬æ ¼å¼                                 â”‚
â”‚         â†“                                                  â”‚
â”‚    å­˜å‚¨æ—¶é—´åºåˆ—æ•°æ®åˆ° TSDB                                  â”‚
â”‚         â†“                                                  â”‚
â”‚    å¯é€šè¿‡ PromQL æŸ¥è¯¢:                                      â”‚
â”‚    http://localhost:9090/api/v1/query?query=...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. Grafana å¯è§†åŒ–                                          â”‚
â”‚                                                             â”‚
â”‚    ç”¨æˆ·è®¿é—®: http://localhost:3000                         â”‚
â”‚         â†“                                                  â”‚
â”‚    æ‰“å¼€ Dashboard: "Tool Runtime Metrics"                  â”‚
â”‚         â†“                                                  â”‚
â”‚    Panel æŸ¥è¯¢ Prometheus:                                  â”‚
â”‚    â”œâ”€â”€ Tool Invocation Rate                                â”‚
â”‚    â”‚   rate(llama_stack_tool_runtime_invocations_total[1m])â”‚
â”‚    â”œâ”€â”€ Success vs Error Rate                               â”‚
â”‚    â”‚   sum by(status)(rate(...[5m]))                       â”‚
â”‚    â”œâ”€â”€ P95 Latency                                         â”‚
â”‚    â”‚   histogram_quantile(0.95, rate(..._bucket[1m]))      â”‚
â”‚    â””â”€â”€ Total Invocations                                   â”‚
â”‚        sum(llama_stack_tool_runtime_invocations_total)     â”‚
â”‚         â†“                                                  â”‚
â”‚    æ¸²æŸ“å›¾è¡¨å’Œé¢æ¿                                           â”‚
â”‚         â†“                                                  â”‚
â”‚    ç”¨æˆ·çœ‹åˆ°å®æ—¶ metrics! ğŸ“Š                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## éªŒè¯å’Œæµ‹è¯•

### 1. éªŒè¯ Provider å·²æ³¨å†Œ

```bash
python -c "
from llama_stack.providers.registry.tool_runtime import available_providers
providers = available_providers()
for p in providers:
    if 'weather' in str(p.provider_type):
        print(f'âœ… Found: {p.provider_type}')
        print(f'   Module: {p.module}')
"
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ… Found: inline::weather
   Module: llama_stack.providers.inline.tool_runtime.weather
```

### 2. éªŒè¯ Server å¯åŠ¨

```bash
# æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:8321/v1/health

# æ£€æŸ¥ telemetry åˆå§‹åŒ–
grep "OpenTelemetry metrics exporter configured" /tmp/llama-stack-weather-metrics.log
```

**é¢„æœŸè¾“å‡º**ï¼š
```
INFO ... OpenTelemetry metrics exporter configured: http://localhost:4318 (interval: 5.0s)
```

### 3. éªŒè¯ Tool Group æ³¨å†Œ

```bash
llama-stack-client toolgroups list
```

**é¢„æœŸè¾“å‡º**ï¼š
```
â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ identifier â”ƒ provider_id     â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ weather    â”‚ weather-runtime â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. æµ‹è¯•å·¥å…·è°ƒç”¨

```python
# test_weather_client.py
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url="http://localhost:8321")

result = client.tool_runtime.invoke_tool(
    tool_name="get_weather",
    kwargs={"city": "Tokyo", "units": "celsius"}
)

print(result.content)
```

**é¢„æœŸè¾“å‡º**ï¼š
```
Weather in Tokyo:
Temperature: 22.1Â°C
Condition: Sunny
Humidity: 65%
Wind Speed: 12.3 km/h
```

### 5. éªŒè¯ Metrics ç”Ÿæˆ

```bash
# ç­‰å¾…å‡ ç§’è®© metrics å¯¼å‡º
sleep 10

# æŸ¥è¯¢ OTLP Collector
curl -s http://localhost:8889/metrics | grep llama_stack_tool_runtime_invocations_total
```

**é¢„æœŸè¾“å‡º**ï¼š
```
llama_stack_tool_runtime_invocations_total{
    tool_name="get_weather",
    tool_group="weather",
    provider="weather-runtime",
    status="success",
    service_name="llama-stack-server"
} 1
```

### 6. éªŒè¯ Prometheus æ•°æ®

```bash
# ç­‰å¾… Prometheus æŠ“å– (15ç§’é—´éš”)
sleep 20

# æŸ¥è¯¢ Prometheus
curl -s 'http://localhost:9090/api/v1/query?query=llama_stack_tool_runtime_invocations_total' | \
  python -m json.tool | grep -A 5 '"tool_name"'
```

**é¢„æœŸè¾“å‡º**ï¼š
```json
{
    "tool_name": "get_weather",
    "status": "success",
    "provider": "weather-runtime"
}
```

### 7. éªŒè¯ Grafana Dashboard

1. æ‰“å¼€æµè§ˆå™¨: http://localhost:3000
2. ç™»å½•: admin / admin
3. å¯¼èˆª: Dashboards â†’ Llama Stack â†’ Tool Runtime Metrics
4. æŸ¥çœ‹é¢æ¿:
   - Tool Invocation Rate
   - Success vs Error Rate
   - Tool Latency P50/P95/P99
   - Total Invocations

---

## å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

```bash
# å¯åŠ¨å®Œæ•´ç³»ç»Ÿ (æ¨è)
./start_weather_with_metrics.sh

# æ‰‹åŠ¨å¯åŠ¨ server (å¸¦ metrics)
export OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4318"
export OTEL_EXPORTER_OTLP_PROTOCOL="http/protobuf"
export OTEL_SERVICE_NAME="llama-stack-server"
export OTEL_METRIC_EXPORT_INTERVAL="5000"
llama stack run weather-stack-config.yaml --port 8321

# æ³¨å†Œ tool group
curl -X POST http://localhost:8321/v1/toolgroups \
  -H "Content-Type: application/json" \
  -d '{"toolgroup_id": "weather", "provider_id": "weather-runtime"}'

# åˆ—å‡º tool groups
llama-stack-client toolgroups list

# æµ‹è¯•å·¥å…·
python test_weather_client.py

# æŸ¥çœ‹ metrics (OTLP Collector)
curl http://localhost:8889/metrics | grep llama_stack_tool

# æŸ¥è¯¢ Prometheus
curl 'http://localhost:9090/api/v1/query?query=llama_stack_tool_runtime_invocations_total'

# æŸ¥çœ‹ server æ—¥å¿—
tail -f /tmp/llama-stack-weather-metrics.log

# åœæ­¢ server
ps aux | grep "llama stack" | grep -v grep
kill <PID>
```

---

## æ€»ç»“

### å…³é”®è¦ç‚¹

1. **Provider ç»“æ„**ï¼š
   - `config.py`: é…ç½®ç±»
   - `weather.py`: å®ç°é€»è¾‘ï¼ˆç»§æ‰¿ ToolRuntime å’Œ ToolGroupsProtocolPrivateï¼‰
   - `__init__.py`: å¯¼å‡º `get_provider_impl` å‡½æ•°

2. **Registry æ³¨å†Œ**ï¼š
   - åœ¨ `src/llama_stack/providers/registry/tool_runtime.py` ä¸­æ·»åŠ  `InlineProviderSpec`
   - æŒ‡å®š `provider_type`, `module`, `config_class`

3. **Tool Group æ³¨å†Œ**ï¼š
   - é…ç½®æ–‡ä»¶ä¸­çš„ `tool_groups` å¯èƒ½ä¸è‡ªåŠ¨ç”Ÿæ•ˆ
   - éœ€è¦é€šè¿‡ API æ‰‹åŠ¨æ³¨å†Œä¸€æ¬¡

4. **Metrics å…³é”®**ï¼š
   - **å¿…é¡»è®¾ç½®** OTEL ç¯å¢ƒå˜é‡æ‰èƒ½å¯¼å‡º metrics
   - Metrics åœ¨ `ToolRuntimeRouter` ä¸­è‡ªåŠ¨æ’æ¡©
   - æ— éœ€ä¿®æ”¹ provider ä»£ç å³å¯è·å¾— metrics

5. **è°ƒè¯•æŠ€å·§**ï¼š
   - æ£€æŸ¥ server æ—¥å¿—ç¡®è®¤ telemetry åˆå§‹åŒ–
   - å…ˆæŸ¥è¯¢ OTLP Collector (port 8889) å†æŸ¥ Prometheus
   - æ³¨æ„ Prometheus æŠ“å–é—´éš” (15ç§’)

### æ–‡ä»¶æ¸…å•

```
llama-stack/
â”œâ”€â”€ src/llama_stack/providers/
â”‚   â”œâ”€â”€ inline/tool_runtime/weather/     # Weather Provider
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ weather.py
â”‚   â””â”€â”€ registry/
â”‚       â””â”€â”€ tool_runtime.py              # å·²ä¿®æ”¹
â”‚
â”œâ”€â”€ weather-stack-config.yaml            # Server é…ç½®
â”œâ”€â”€ start_weather_with_metrics.sh        # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ test_weather_client.py               # æµ‹è¯•å®¢æˆ·ç«¯
â””â”€â”€ WEATHER_TOOL_COMPLETE_GUIDE.md       # æœ¬æ–‡æ¡£
```

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼ğŸ¦™ğŸŒ¤ï¸ğŸ“Š**

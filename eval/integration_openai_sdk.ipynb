{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki7E44X5ViQB"
      },
      "source": [
        "---\n",
        "description: Drop-in replacement of OpenAI SDK to get full observability in Langfuse by changing only the import\n",
        "category: Integrations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfMAzJYcirtK"
      },
      "source": [
        "# Cookbook: OpenAI Integration (Python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0A389k2irtK"
      },
      "source": [
        "This is a cookbook with examples of the Langfuse Integration for OpenAI (Python).\n",
        "\n",
        "Follow the [integration guide](https://langfuse.com/docs/integrations/openai/get-started) to add this integration to your OpenAI project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq04G_FSWjF-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYoil3FcOIQt"
      },
      "source": [
        "The integration is compatible with OpenAI SDK versions `>=0.27.8`. It supports async functions and streaming for OpenAI SDK versions `>=1.0.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hVOOiBtUPtOO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langfuse in /Users/gyliu/venvlf/lib/python3.10/site-packages (2.21.1)\n",
            "Requirement already satisfied: openai in /Users/gyliu/venvlf/lib/python3.10/site-packages (1.14.3)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from langfuse) (1.16.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from langfuse) (23.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from langfuse) (2.4.0)\n",
            "Requirement already satisfied: httpx<1.0,>=0.15.4 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from langfuse) (0.24.1)\n",
            "Requirement already satisfied: chevron<0.15.0,>=0.14.0 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from langfuse) (0.14.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=2.2.1 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from langfuse) (2.2.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from openai) (1.8.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: sniffio in /Users/gyliu/venvlf/lib/python3.10/site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /Users/gyliu/venvlf/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /Users/gyliu/venvlf/lib/python3.10/site-packages (from httpx<1.0,>=0.15.4->langfuse) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from httpx<1.0,>=0.15.4->langfuse) (0.17.3)\n",
            "Requirement already satisfied: pydantic-core==2.10.0 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.6.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1.0,>=0.15.4->langfuse) (0.14.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install langfuse openai --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ldSEJ0bAP4sj"
      },
      "outputs": [],
      "source": [
        "# instead of: import openai\n",
        "from langfuse.openai import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G8qkHd8oK_o9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For debugging, checks the SDK connection with the server. Do not use in production as it adds latency.\n",
        "from langfuse.openai import auth_check\n",
        "\n",
        "auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ovnAAdbaLmD"
      },
      "source": [
        "## Examples\n",
        "\n",
        "### Chat completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c8RhokKUP9I0"
      },
      "outputs": [],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAqxBgOqKTzO"
      },
      "source": [
        "### Chat completion (streaming)\n",
        "\n",
        "Simple example using the OpenAI streaming functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "b9gRlb2rKTaA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure thing! Why did the scarecrow win an award? Because he was outstanding in his field!None"
          ]
        }
      ],
      "source": [
        "completion = openai.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a professional comedian.\"},\n",
        "      {\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in completion:\n",
        "  print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2pvm0qLKg7Q"
      },
      "source": [
        "### Chat completion (async)\n",
        "\n",
        "Simple example using the OpenAI async client. It takes the Langfuse configurations either from the environment variables or from the attributes on the `openai` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hggwggv_MKpV"
      },
      "outputs": [],
      "source": [
        "from langfuse.openai import AsyncOpenAI\n",
        "\n",
        "async_client = AsyncOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZIUKD8Z3KmvQ"
      },
      "outputs": [],
      "source": [
        "completion = await async_client.chat.completions.create(\n",
        "  name=\"test-chat\",\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"},\n",
        "      {\"role\": \"user\", \"content\": \"1 + 100 = \"}],\n",
        "  temperature=0,\n",
        "  metadata={\"someMetadataKey\": \"someValue\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4iJpqYQirtM"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
        "\n",
        "![Chat completion](https://langfuse.com/images/docs/openai-chat.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7CtCNzaSrn"
      },
      "source": [
        "### Functions\n",
        "\n",
        "Simple example using Pydantic to generate the function schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jJfBdHowaRgs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /Users/gyliu/venvlf/lib/python3.10/site-packages (2.4.0)\n",
            "Collecting pydantic\n",
            "  Using cached pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
            "Collecting pydantic-core==2.16.3\n",
            "  Using cached pydantic_core-2.16.3-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/gyliu/venvlf/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
            "Installing collected packages: pydantic-core, pydantic\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.10.0\n",
            "    Uninstalling pydantic_core-2.10.0:\n",
            "      Successfully uninstalled pydantic_core-2.10.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.4.0\n",
            "    Uninstalling pydantic-2.4.0:\n",
            "      Successfully uninstalled pydantic-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlmodel 0 requires pydantic[email]<=2.4,>=2.1.1, but you have pydantic 2.6.4 which is incompatible.\n",
            "langflow 0.6.0a0 requires cohere<5.0.0,>=4.32.0, but you have cohere 5.1.2 which is incompatible.\n",
            "langflow 0.6.0a0 requires langchain<0.1.0,>=0.0.327, but you have langchain 0.1.13 which is incompatible.\n",
            "langflow 0.6.0a0 requires langfuse<2.0.0,>=1.1.11, but you have langfuse 2.21.1 which is incompatible.\n",
            "langflow 0.6.0a0 requires openai<0.28.0,>=0.27.8, but you have openai 1.14.3 which is incompatible.\n",
            "langflow 0.6.0a0 requires orjson==3.9.3, but you have orjson 3.9.15 which is incompatible.\n",
            "langflow 0.6.0a0 requires tiktoken<0.6.0,>=0.5.0, but you have tiktoken 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-2.6.4 pydantic-core-2.16.3\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pydantic --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2gA-zGk7VYYp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/m3/cz7sy7cd4v7_mklwrfjs45wm0000gn/T/ipykernel_99748/2496491748.py:7: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
            "  schema = StepByStepAIResponse.schema() # returns a dict like JSON schema\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class StepByStepAIResponse(BaseModel):\n",
        "    title: str\n",
        "    steps: List[str]\n",
        "schema = StepByStepAIResponse.schema() # returns a dict like JSON schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ORtNcN4-afDC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/m3/cz7sy7cd4v7_mklwrfjs45wm0000gn/T/ipykernel_99748/162860978.py:12: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.4/migration/\n",
            "  \"parameters\": StepByStepAIResponse.schema()\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "response = openai.chat.completions.create(\n",
        "    name=\"test-function\",\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    messages=[\n",
        "       {\"role\": \"user\", \"content\": \"Explain how to assemble a PC\"}\n",
        "    ],\n",
        "    functions=[\n",
        "        {\n",
        "          \"name\": \"get_answer_for_user_query\",\n",
        "          \"description\": \"Get user answer in series of steps\",\n",
        "          \"parameters\": StepByStepAIResponse.schema()\n",
        "        }\n",
        "    ],\n",
        "    function_call={\"name\": \"get_answer_for_user_query\"}\n",
        ")\n",
        "\n",
        "output = json.loads(response.choices[0].message.function_call.arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurrm-Ntp24O"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your generation.\n",
        "\n",
        "![Function](https://langfuse.com/images/docs/openai-function.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su1OaQq3rPPh"
      },
      "source": [
        "### Group multiple generations into a single trace\n",
        "\n",
        "Many applications require more than one OpenAI call. The `@observe()` decorator allows to nest all LLM calls of a single API invocation into the same `trace` in Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zMDVxzS1ltWU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the heart of the Balkans, she stands so proud,\n",
            "A city of shadows and light, both silent and loud.\n",
            "With ancient streets that weave and wind,\n",
            "Through history's tapestry, a story to find.\n",
            "\n",
            "Sofia, the city of red-roofed homes,\n",
            "Where saints and sinners freely roam.\n",
            "The Alexander Nevsky Cathedral so grand,\n",
            "A beacon of faith in this ancient land.\n",
            "\n",
            "Among the bustling markets and café's cheer,\n",
            "The spirit of Sofia is ever near.\n",
            "From Vitosha Mountain, her guardian high,\n",
            "To the Serdika ruins where old worlds lie.\n",
            "\n",
            "The heartbeat of Bulgaria, a city so alive,\n",
            "In her cobblestone streets, stories thrive.\n",
            "With each passing moment, a new tale begun,\n",
            "Sofia, eternal, beneath the Balkan sun.\n",
            "\n",
            "Her people, vibrant, diverse and strong,\n",
            "In unity and resilience, they belong.\n",
            "A city of contrasts, old and new,\n",
            "Sofia, forever in my heart, I'll hold\n"
          ]
        }
      ],
      "source": [
        "from langfuse.openai import openai\n",
        "from langfuse.decorators import observe\n",
        "\n",
        "@observe() # decorator to automatically create trace and nest generations\n",
        "def main(country: str, user_id: str, **kwargs) -> str:\n",
        "    # nested generation 1: use openai to get capital of country\n",
        "    capital = openai.chat.completions.create(\n",
        "      name=\"geography-teacher\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
        "          {\"role\": \"user\", \"content\": country}],\n",
        "      temperature=0,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # nested generation 2: use openai to write poem on capital\n",
        "    poem = openai.chat.completions.create(\n",
        "      name=\"poet\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
        "          {\"role\": \"user\", \"content\": capital}],\n",
        "      temperature=1,\n",
        "      max_tokens=200,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    return poem\n",
        "\n",
        "# run main function and let Langfuse decorator do the rest\n",
        "print(main(\"Bulgaria\", \"admin\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehx2NZuIrPPh"
      },
      "source": [
        "Go to https://cloud.langfuse.com or your own instance to see your trace.\n",
        "\n",
        "![Trace with multiple OpenAI calls](https://langfuse.com/images/docs/openai-trace-grouped.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HeMqTWgK4xL"
      },
      "source": [
        "#### Fully featured: Interoperability with Langfuse SDK\n",
        "\n",
        "The `trace` is a core object in Langfuse and you can add rich metadata to it. See [Python SDK docs](https://langfuse.com/docs/sdk/python#traces-1) for full documentation on this.\n",
        "\n",
        "Some of the functionality enabled by custom traces:\n",
        "- custom name to identify a specific trace-type\n",
        "- user-level tracking\n",
        "- experiment tracking via versions and releases\n",
        "- custom metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "28to65wpK4xL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the heart of the Balkans, where history meets modernity,\n",
            "Lies a city of beauty, Sofia, a place of pure serenity.\n",
            "With ancient ruins whispering tales of days long gone,\n",
            "And vibrant street art that dances with the dawn.\n",
            "\n",
            "A melting pot of cultures, where East meets West,\n",
            "Sofia's charm will put your wandering soul to the test.\n",
            "The rhythm of the city pulses through its veins,\n",
            "As vibrant markets bustle and soothing fountains reign.\n",
            "\n",
            "Beneath the shadow of Vitosha, the mountain so grand,\n",
            "Sofia stands proud, a jewel in Bulgaria's hand.\n",
            "With its grand cathedrals and majestic domes,\n",
            "It's a city that calls you to wander and roam.\n",
            "\n",
            "From the bustling boulevards to quiet cobbled lanes,\n",
            "Sofia's spirit will stir in your heart like gentle rains.\n",
            "So come, wanderer, and let the city reveal,\n",
            "The magic and wonder that its streets conceal.\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "from langfuse.openai import openai\n",
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "@observe() # decorator to automatically create trace and nest generations\n",
        "def main(country: str, user_id: str, **kwargs) -> str:\n",
        "    # nested generation 1: use openai to get capital of country\n",
        "    capital = openai.chat.completions.create(\n",
        "      name=\"geography-teacher\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a Geography teacher helping students learn the capitals of countries. Output only the capital when being asked.\"},\n",
        "          {\"role\": \"user\", \"content\": country}],\n",
        "      temperature=0,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # nested generation 2: use openai to write poem on capital\n",
        "    poem = openai.chat.completions.create(\n",
        "      name=\"poet\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a poet. Create a poem about a city.\"},\n",
        "          {\"role\": \"user\", \"content\": capital}],\n",
        "      temperature=1,\n",
        "      max_tokens=200,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # rename trace and set attributes (e.g., medatata) as needed\n",
        "    langfuse_context.update_current_trace(\n",
        "        name=\"City poem generator\",\n",
        "        session_id=\"1234\",\n",
        "        user_id=user_id,\n",
        "        tags=[\"tag1\", \"tag2\"],\n",
        "        public=True,\n",
        "        metadata = {\n",
        "        \"env\": \"development\",\n",
        "        },\n",
        "        release = \"v0.0.21\"\n",
        "    )\n",
        "\n",
        "    return poem\n",
        "\n",
        "# create random trace_id, could also use existing id from your application, e.g. conversation id\n",
        "trace_id = str(uuid.uuid4())\n",
        "\n",
        "# run main function, set your own id, and let Langfuse decorator do the rest\n",
        "print(main(\"Bulgaria\", \"admin\", langfuse_observation_id=trace_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3jxed-VrPPi"
      },
      "source": [
        "### Programmatically add scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMO6tn53rPPi"
      },
      "source": [
        "You can add [scores](https://langfuse.com/docs/scores) to the trace, to e.g. record user feedback or some programmatic evaluation. Scores are used throughout Langfuse to filter traces and on the dashboard. See the docs on scores for more details.\n",
        "\n",
        "The score is associated to the trace using the `trace_id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J0argbJhrPPi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langfuse.client.StatefulClient at 0x10cc062f0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langfuse import Langfuse\n",
        "from langfuse.decorators import langfuse_context, observe\n",
        "\n",
        "langfuse = Langfuse()\n",
        "\n",
        "@observe() # decorator to automatically create trace and nest generations\n",
        "def main():\n",
        "    # get trace_id of current trace\n",
        "    trace_id = langfuse_context.get_current_trace_id()\n",
        "\n",
        "    # rest of your application ...\n",
        "\n",
        "    return \"res\", trace_id\n",
        "\n",
        "# execute the main function to generate a trace\n",
        "_, trace_id = main()\n",
        "\n",
        "# Score the trace from outside the trace context\n",
        "langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"my-score-name\",\n",
        "    value=1\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
